<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
    <script>
MathJax = {
  loader: {
    load: ["[attrLabel]/attr-label.js"],
    paths: { attrLabel: "../resource" },
  },
  tex: { packages: { "[+]": ["attr-label"] },
   inlineMath: [['$', '$']]
   },
   options: {
    enableAssistiveMml: false
  },
};
    </script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script src="../resource/d3.min.js"></script>
    <script src="../resource/svg.min.js"></script>
    <script type="text/javascript" src='../resource/paper.js'></script>
    <link rel="stylesheet" href="../resource/paper.css">
</head>
<script>
const iheartla_data = JSON.parse('{"equations":[{"name":"first", "parameters":[{"sym":"m", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"ğ‘", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"10"}},{"sym":"$ğ‘“_{ğ‘ 0}$", "type_info":{"type": "scalar"}},{"sym":"a", "type_info":{"type": "scalar"}},{"sym":"q", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}}], "definition":[{"sym":"ğ‘”", "type_info":{"type": "function", "params":[{"type": "vector", "element":{"type": "scalar"}, "rows":"n"},{"type": "vector", "element":{"type": "scalar"}, "rows":"n"},{"type": "scalar"},{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "matrix", "element":{"type": "scalar"}, "rows":"n", "cols":"2"}}},{"sym":"Î¨", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"ğœ", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"ğœ", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"A", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"d", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"ğ‘ ", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}}], "local_func":[{"name":"ğ‘”", "parameters":[{"sym":"x", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"n"}},{"sym":"$x_0$", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"n"}},{"sym":"ğœƒ", "type_info":{"type": "scalar"}},{"sym":"ğœ", "type_info":{"type": "scalar"}},{"sym":"$ğ‘“_ğ‘ $", "type_info":{"type": "scalar"}}]},{"name":"Î¨", "parameters":[{"sym":"ğ‘’", "type_info":{"type": "scalar"}},{"sym":"$ğ‘“_ğ‘ $", "type_info":{"type": "scalar"}}]},{"name":"ğœ", "parameters":[{"sym":"$ğ‘“_ğ‘ $", "type_info":{"type": "scalar"}}]},{"name":"ğœ", "parameters":[{"sym":"$ğ‘“_ğ‘ $", "type_info":{"type": "scalar"}}]},{"name":"A", "parameters":[{"sym":"ğ‘’", "type_info":{"type": "scalar"}}]},{"name":"d", "parameters":[{"sym":"L", "type_info":{"type": "scalar"}}]},{"name":"ğ‘ ", "parameters":[{"sym":"ğ‘’", "type_info":{"type": "scalar"}},{"sym":"$ğ‘“_ğ‘ $", "type_info":{"type": "scalar"}}]}], "source":"cos, sin from trigonometry\\n\\nğ‘”(x,$x_0$, ğœƒ,ğœ,$ğ‘“_ğ‘ $) = exp(-||x-$x_0$||^2/(2ğœ^2)) cos(2Ï€$ğ‘“_ğ‘ $x â‹…[cos(ğœƒ) sin(ğœƒ)]) where x: â„^n,$x_0$: â„^n,$ğ‘“_ğ‘ $: â„, ğœ : â„, ğœƒ : â„\\n\\n\\nÎ¨(ğ‘’, $ğ‘“_ğ‘ $)= m(0, ğ‘â‚€ ğœ($ğ‘“_ğ‘ $) +ğ‘â‚ ğœ($ğ‘“_ğ‘ $)+ğ‘â‚‚ + (ğ‘â‚ƒ ğœ($ğ‘“_ğ‘ $)^2 + ğ‘â‚„ ğœ($ğ‘“_ğ‘ $) +ğ‘â‚…)â‹… ğœ($ğ‘“_ğ‘ $)ğ‘’ + (ğ‘â‚† ğœ($ğ‘“_ğ‘ $)^2 +ğ‘â‚‡ ğœ($ğ‘“_ğ‘ $) + ğ‘â‚ˆ)â‹…ğœ($ğ‘“_ğ‘ $)ğ‘’^2) where $ğ‘“_ğ‘ $ : â„, ğ‘’: â„\\n\\nğœ($ğ‘“_ğ‘ $) = exp(ğ‘â‚‰ ğœ($ğ‘“_ğ‘ $)) - 1 where $ğ‘“_ğ‘ $ : â„\\nğœ($ğ‘“_ğ‘ $) = m(log($ğ‘“_ğ‘ $)-log($ğ‘“_{ğ‘ 0}$), 0) where $ğ‘“_ğ‘ $ : â„\\nwhere\\nm: â„, â„ -> â„\\nğ‘: â„^10\\n$ğ‘“_{ğ‘ 0}$: â„\\n\\n\\nA(ğ‘’)= ln(64) 2.3/(0.106â‹…(ğ‘’+2.3) )  where ğ‘’ : â„\\n\\n\\n\\nd(L)= 7.75-5.75((La/846)^0.41/((La/846)^0.41 + 2))  where L : â„\\nwhere \\na : â„\\n\\n\\n\\nğ‘ (ğ‘’,$ğ‘“_ğ‘ $) = ğœ($ğ‘“_ğ‘ $)(q_0 ğ‘’^2 + q_1 ğ‘’) + q_2  where $ğ‘“_ğ‘ $ : â„, ğ‘’: â„\\n\\n\\nwhere \\nq : â„^3\\n\\n" }] }');
const sym_data = JSON.parse('{"m":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"first", "is_defined":false, "used_equations":[]}],"ğ‘":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"10"}, "def_module":"first", "is_defined":false, "used_equations":[]}],"$ğ‘“_{ğ‘ 0}$":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"first", "is_defined":false, "used_equations":[]}],"a":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"first", "is_defined":false, "used_equations":[]}],"q":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}, "def_module":"first", "is_defined":false, "used_equations":[]}]}');
window.onload = parseAllSyms;
function reportWindowSize() {
  var arrows = document.querySelectorAll(".arrow");
  if (arrows) {
    for (var i = arrows.length - 1; i >= 0; i--) {
      var arrow = arrows[i];
      var body = document.querySelector("body");
      var style = window.getComputedStyle(body);
      var curOffset = parseInt(style.marginLeft, 10)
      var oldOffset = arrow.getAttribute('offset');
      arrow.setAttribute('offset', curOffset);
      // console.log(`oldOffset:${oldOffset}, curOffset:${curOffset}`);
      var arrowStyle = window.getComputedStyle(arrow); 
      var arrowOffset = parseInt(document.querySelector(".arrow").style.marginLeft, 10)
      arrow.style.marginLeft = `${arrowOffset+curOffset-oldOffset}px`;
      var newWidth = parseInt(style.width, 10) + parseInt(style.marginLeft, 10) + parseInt(style.marginRight, 10);
      arrow.style.width = `${newWidth}px`;
      arrow.style.height = style.height; 
      // console.log(`arrow.style.width:${arrow.style.width}, arrow.style.height:${arrow.style.height}`)
    }
  }
}
window.onresize = reportWindowSize;
document.addEventListener("click", function(evt){
    resetState();
});

</script>
<body>
<img src="../resource/glossary.png" id="glossary" alt="glossary" width="22" height="28"><br>
<div class='title'>A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker Fusion and its Applications to Foveated Graphics</div><div class='author'>BROOKE KRAJANCICH, Stanford University</div><p class='abstract'>Virtual and augmented reality (VR/AR) displays strive to provide a resolution, framerate and field of view that matches the perceptual capabilities of the human visual system, all while constrained by limited compute budgets and transmission bandwidths of wearable computing systems. Foveated graphics techniques have emerged that could achieve these goals by exploiting the falloff of spatial acuity in the periphery of the visual field. However, consid- erably less attention has been given to temporal aspects of human vision, which also vary across the retina. This is in part due to limitations of current eccentricity-dependent models of the visual system. We introduce a new model, experimentally measuring and computationally fitting eccentricity- dependent critical flicker fusion thresholds jointly for both space and time. In this way, our model is unique in enabling the prediction of temporal infor- mation that is imperceptible for a certain spatial frequency, eccentricity, and range of luminance levels. We validate our model with an image quality user study, and use it to predict potential bandwidth savings 7Ã— higher than those afforded by current spatial-only foveated models. As such, this work forms the enabling foundation for new temporally foveated graphics techniques.</p><ul><li><a href='#introduction'>1&nbsp;INTRODUCTION</a></li><li><a href='#related-work'>2&nbsp;RELATED WORK</a><ul><li><a href='#perceptual-models'>2.1&nbsp;Perceptual Models</a></li><li><a href='#foveated-graphics'>2.2&nbsp;Foveated Graphics</a></li></ul></li><li><a href='#estimating-flicker-fusion-thresholds'>3&nbsp;ESTIMATING FLICKER FUSION THRESHOLDS</a><ul><li><a href='#display-prototype'>3.1&nbsp;Display Prototype</a></li><li><a href='#user-study'>3.2&nbsp;User Study</a></li></ul></li><li><a href='#an-eccentricity-dependent-perceptual-model-for-spatio-temporal-flicker-fusion'>4&nbsp;AN ECCENTRICITY-DEPENDENT PERCEPTUAL MODEL FOR SPATIO-TEMPORAL FLICKER FUSION</a><ul><li><a href='#model-fitting'>4.1&nbsp;Model Fitting</a></li><li><a href='#extension-for-high-spatial-frequencies'>4.2&nbsp;Extension for High Spatial Frequencies</a></li><li><a href='#adaptation-luminance'>4.3&nbsp;Adaptation luminance</a></li></ul></li><li><a href='#experimental-validation'>5&nbsp;EXPERIMENTAL VALIDATION</a></li><li><a href='#analyzing-bandwidth-considerations'>6&nbsp;ANALYZING BANDWIDTH CONSIDERATIONS</a></li><li><a href='#discussion'>7&nbsp;DISCUSSION</a></li><li><a href='#acknowledgments'>8&nbsp;ACKNOWLEDGMENTS</a></li><li><a href='#references'>9&nbsp;REFERENCES</a></li></ul>

<h1 id='introduction'>1&nbsp;INTRODUCTION</h1><p>Emerging virtual and augmented reality (VR/AR) systems have un- locked new user experiences by offering unprecedented levels of immersion. With the goal of showing digital content that is indistin- guishable from the real world, VR/AR displays strive to match the perceptual limits of human vision. That is, a resolution, framerate, and field of view (FOV) that matches what the human eye can per- ceive. However, the required bandwidths for graphics processing units to render such high-resolution and high-framerate content in interactive applications, for networked systems to stream, and for displays and their interfaces to transmit and present this data is far from achievable with current hardware or standards.</p>
<p>Foveated graphics techniques have emerged as one of the most promising solutions to overcome these challenges. These approaches use the fact that the acuity of human vision is eccentricity depen- dent, i.e., acuity is highest in the fovea and drops quickly towards the periphery of the visual field. In a VR/AR system, this can be ex- ploited using gaze-contingent rendering, shading, compression, or display (see Sec. 2). While all of these methods build on the insight that acuity, or spatial resolution, varies across the retina, common wisdom also suggests that so does temporal resolution. In fact it may be anti-foveated in that it is higher in the periphery of the visual field than in the fovea. This suggests that further bandwidth savings, well beyond those offered by todayâ€™s foveated graphics approaches, could be enabled by exploiting such perceptual limitations with novel gaze-contingent hardware or software solutions. To the best of our knowledge, however, no perceptual model for eccentricity- dependent spatio-temporal aspects of human vision exists today, hampering the progress of such foveated graphics techniques.</p>
<p>The primary goal of our work is to experimentally measure user data and computationally fit models that adequately describe the eccentricity-dependent spatio-temporal aspects of human vision. Specifically, we design and conduct user studies with a custom, high- speed VR display that allow us to measure data to model critical flicker fusion (CFF) in a spatially-modulated manner. In this paper, we operationally define CFF as a measure of spatio-temporal flicker fusion thresholds. As such, and unlike current models of foveated vision, our model is unique in predicting what temporal information may be imperceptible for a certain eccentricity, spatial frequency, and luminance.</p>
<p>Using our model, we predict potential bandwidth savings of fac- tors up to 3,500Ã— over unprocessed visual information and 7Ã— over existing foveated models that do not account for the temporal char- acteristics of human vision.</p>
<p>Specifically we make the following contributions:</p>
<ul>
<li>We design and conduct user studies to measure and validate eccentricity-dependent spatio-temporal flicker fusion thresh- olds with a custom display.</li>
<li>We fit several variants of an analytic model to these data and also extrapolate the model beyond the space of our mea- surements using data provided in the literature, including an extension for varying luminance.</li>
<li>We analyze bandwidth considerations and demonstrate that our model may afford significant bandwidth savings for fove- ated graphics.</li>
</ul>
<p>Overview of Limitations. The primary goals of this work are to develop a perceptual model and to demonstrate its potential benefits to foveated graphics. However, we are not proposing new foveated rendering algorithms or specific compression schemes that directly use this model.</p>
<h1 id='related-work'>2&nbsp;RELATED WORK</h1><h2 id='perceptual-models'>2.1&nbsp;Perceptual Models</h2><p>The human visual system (HVS) is limited in its ability to sense<br />
variations in light intensity over space and time. Furthermore, these abilities vary across the visual field. The drop in spatial sensitivity with eccentricity has been well studied, attributed primarily to the drop in retinal ganglion cell densities [Curcio and Allen 1990] but also lens aberrations [Shen et al. 2010; Thibos et al. 1987]. This is often described by the spatial contrast sensitivity function (sCSF), defined as the inverse of the smallest contrast of sinusoidal grating that can be detected at each spatial frequency [Robson 1993]. Spatial resolution, or acuity, of the visual system is then defined as the highest spatial frequency that can be seen, when contrast is at its maximum value of 1 or the log of the sCSF is zero.</p>
<p>On the contrary, temporal sensitivity has been observed to peak in the periphery, somewhere between 20 âˆ’ 50â—¦ eccentricity [Hartmann et al. 1979; Rovamo and Raninen 1984; Tyler 1987], attributed to faster cone cell responses in the mid visual field by Sinha et al. [2017]. Analogous to spatial sensitivity, temporal sensitivity is often de- scribed by the temporal contrast sensitivity function (tCSF) and de- tection at each temporal frequency. CFF thresholds then define the temporal resolution of the visual system. While Davis et al. [2015] recently showed that retinal image decomposition due to saccades in specific viewing conditions can elicit a flicker sensation beyond this limit, in our work we model only fixated sensitivities and highlight that further modeling would be needed to incorporate gaze-related effects (see Sec. 7).</p>
<p>While related, CFF and tCSF are used by vision scientists to quan- tify slightly different aspects of human vision. The CFF is considered to be a measure of conscious visual detection dependent on the tem- poral resolution of visual neurons, since at the CFF threshold, an identical flickering stimulus varies in percept from flickering to stable [Carmel et al. 2006]. While contrast sensitivity is considered to be more of a measure of visual discrimination, as evidenced by the sinusoidal grating used in its measure requiring orientation recognition [Pelli and Bex 2013].</p>
<p>Several studies measure datapoints for these functions indepen- dently [de Lange Dzn 1958; Eisen-Enosh et al. 2017; Van Nes and Bouman 1967], across eccentricity [Hartmann et al. 1979; Koen- derink et al. 1978a], as a function of luminance [Koenderink et al. 1978b] and color [Davis et al. 2015], however few present quantita- tive models, possibly due to sparse sampling of the measured data. Additionally, data is often captured at luminances very different from typical VR displays. The Ferryâ€“Porter [Tyler and Hamer 1990] and Granitâ€“Harper [Rovamo and Raninen 1988] laws are exceptions to this, describing CFF as increasing linearly with log retinal illu- minance and log stimulus area, respectively. Subsequent work by Tyler et al. [1993] showed that the Ferry-Porter law also extends to higher eccentricities. Koenderink et al. [1978] also derived a complex model for sCSF across eccentricity for arbitrary spatial patterns.</p>
<p>However, spatial and temporal sensitivity are not independent. This relationship can be captured by varying both spatial and tem- poral frequency to obtain the spatio-temporal contrast sensitivity function (stCSF) [Robson 1966]. While Kelly [1979] was the first to fit an analytical function to stCSF data, this was limited to the fovea and a single luminance. Similarly, Watson et al. [2016] devised the pyramid of visibility, a simplified model that can be used if only higher frequencies are relevant. While also modeling luminance dependence, this model was again not applicable to higher eccen- tricities. Subsequent work saw the refitting of this model for higher eccentricity data but the original equation was simplified to only consider stationary content and the prediction of sCSF [Watson 2018].</p>
<p>To the best of our knowledge, however, there is no unified model parameterized by all axes of interest, as illustrated in Table 1. With this work, we address this gap, measuring and fitting the first model that describes CFF in terms of spatial frequency, eccentricity and luminance. In particular, we measure CFF rather than stCSF to be more conservative in modeling the upper limit of human perception, for use in foveated graphics applications.</p>
<h2 id='foveated-graphics'>2.2&nbsp;Foveated Graphics</h2><p>Foveated graphics encompasses a plethora of techniques that is en- abled by eye tracking (see e.g. [Duchowski et al. 2004; Koulieris et al. 2019] for surveys on this topic). Knowing where the user fixates allows for manipulation of the visual data to improve perceptual realism [Krajancich et al. 2020; Mauderer et al. 2014; Padmanaban et al. 2017], or save bandwidth [Albert et al. 2017; Arabadzhiyska et al. 2017], by exploiting the drastically varying acuity of human vision over the retina. The most prominent approach that does the latter is perhaps foveated rendering, where images and videos are rendered, transmitted, or displayed with spatially varying res- olutions without affecting the perceived image quality [Friston et al. 2019; Geisler and Perry 1998; Guenter et al. 2012; Kaplanyan et al. 2019; Patney et al. 2016; Sun et al. 2017]. These methods pri- marily exploit spatial aspects of eccentricity-dependent human vi- sion, but Tursun et al. [2019] recently expanded this concept by considering local luminance contrast across the retina, Xiao et al. [2020] additionally consider temporal coherence, and Kim et al. [2019] showed hardware-enabled solutions for foveated displays. Related approaches also use gaze-contingent techniques to reduce bit-depth [McCarthy et al. 2004] and shading or level-of-detail [Lue- bke and Hallen 2001; Murphy and Duchowski 2001; Ohshima et al. 1996] in the periphery. All of these foveated graphics techniques pri- marily aim at saving bandwidth of the graphics system by reducing the number of vertices or fragments a graphics processing unit has to sample, raytrace, shade, or transmit to the display. To the best of our knowledge, none of these methods exploit the eccentricity- dependent temporal characteristics of human vision, perhaps be- cause no existing model of human perception accounts for these in a principled manner (see Tab. 1). The goal of this paper is to develop such a model for eccentricity-dependent spatio-temporal flicker fusion that could enable significant bandwidth savings for all of the aforementioned techniques well beyond those enabled by existing eccentricity-dependent acuity models.</p>
<p>Many approaches in graphics, such as frame interpolation, tem- poral upsampling [Chen et al. 2005; Denes et al. 2019; Didyk et al. 2010; Scherzer et al. 2012] and multi-frame rate rendering and dis- play [Springer et al. 2007], do account for the limited temporal resolution of human vision. However, spatial and temporal sensi- tivities are not independent. Spatio-temporal video manipulation is common in foveated compression (e.g., [Ho et al. 2005; Wang et al. 2003]) while other studies have investigated how trading one for the other effects task performance, such as in first-person shoot- ers [Claypool and Claypool 2007, 2009]. Denes et al. [2020] recently studied spatio-temporal resolution tradeoffs with perceptual models and applications to VR. However, none of these approaches rely on eccentricity-dependent models of spatio-temporal vision, which could further enhance their performance.</p>
<h1 id='estimating-flicker-fusion-thresholds'>3&nbsp;ESTIMATING FLICKER FUSION THRESHOLDS</h1><p>To develop an eccentricity-dependent model of flicker fusion, we need a display that is capable of showing stimuli at a high fram- erate and over a wide FOV. In this section, we first describe a cus- tom high-speed VR display that we built to support these require- ments. We then proceed with a detailed discussion of the user study we conducted and the resulting values for eccentricity and spatial frequencyâ€“dependent CFF we estimated.</p>
<h2 id='display-prototype'>3.1&nbsp;Display Prototype</h2><p>Our prototype display is designed in a near-eye display form factor to support a wide FOV. As shown in Figure 2(a), we removed the back panel of a View-Master Deluxe VR Viewer and mounted a semi- transparent optical diffuser (Edmund Optics #47-679) instead of a display panel, which serves as a projection screen. This View-Master was fixed to an SR Research headrest to allow users to comfortably view stimuli for extended periods of time. To support a sufficiently high framerate, we opted for a Digital Light Projector (DLP) unit (Texas Instruments DLP3010EVM-LC Evaluation Board) that rear- projects images onto the diffuser towards the viewer. A neutral density (ND16) filter was placed in this light path to reduce the brightness to an eye safe level, measured to be 380 cd/m2 at peak.</p>
<p>The DLP has a resolution of 1280 Ã— 720, and a maximum frame rate of 1.5 kHz for 1-bit video, 360 Hz for 8-bit monochromatic video, or 120 Hz for 24-bit RGB video. We positioned the projector such that the image matched the size of the conventional View-Master display. Considering the magnification of the lenses, this display provides a pixel pitch of 0.1â€™ (arc minutes) and a monocular FOV of 80â—¦ horizontally and 87â—¦ vertically.</p>
<p>To display stimuli for our user study, we used the graphical user interface provided by Texas Instruments to program the DLP to the 360 Hz 8-bit grayscale mode, deemed sufficient for our measure- ments of CFF, which unlike CSF, does not require precise contrast tuning. The DLP was unable to support the inbuilt red, green and blue light-emitting diodes (LEDs) being on simultaneously, so we chose to use a single LED to minimize the possibility of artifacts from temporally multiplexing colors. Furthermore, since the HVS is most sensitive to mid-range wavelengths, the green LED (OSRAM: LE CG Q8WP) of peak wavelength 520 nm and a 100 nm full width at half maximum, was chosen so as to most conservatively measure the CFF thresholds. We used Pythonâ€™s PsychoPy toolbox [Peirce 2007] and a custom shader to stream frames to the display by en- coding them into the required 24-bit RGB format that is sent to the DLP via HDMI.</p>
<p>Stimuli. The flicker fusion model we wish to acquire could be parameterized by spatial frequency, rotation angle, eccentricity (i.e., distance from the fovea), direction from the fovea (i.e., temporal, nasal, etc.) and other parameters. A naive approach may sample across all of these dimensions, but due to the fact that each datapoint needs to be recorded for multiple subjects and for many temporal frequencies per subject to determine the respective CFFs, this seems infeasible. Therefore, similar to several previous studies [Allen and Hess 1992; Eisen-Enosh et al. 2017; Hartmann et al. 1979], we make the following assumptions to make data acquisition tractable:</p>
<ul>
<li>(1) The left and right eyes exhibit the same sensitivities and monocular and binocular viewing conditions are equivalent. Thus, we display the stimuli monocularly to the right eye by blocking the left side of the display.</li>
<li>(2) Sensitivity is rotationally symmetric around the fovea, i.e. independent of nasal, temporal, superior, and inferior direc- tion, thus being only a function of absolute distance from the fovea. It is therefore sufficient to measure stimuli only along the temporal direction starting from the fovea.</li>
<li>(3) Sensitivity is orientation independent. Thus, the rotation an- gle of the test pattern is not significant.</li>
</ul>
<p>These assumptions allow us to reduce the sample space to only two dimensions: eccentricity ğ‘’ and spatial frequency ğ‘“ğ‘  . Later we also analyze retinal illuminance ğ‘™ as an additional factor.</p>
<p>Another fact to consider is that an eccentricity-dependent model that varies with spatial frequency must adhere to the uncertainty principle. That is, low spatial frequencies cannot be well localized in eccentricity. For example, the lowest spatial frequency of 0 cpd is a stimulus that is constant across the entire retina whereas very high spatial frequencies can be well localized in eccentricity. This behavior is appropriately modeled by wavelets. As such, we select our stimuli to be a set of 2D Gabor wavelets. As a complex sinusoid modulated by a Gaussian envelope, these wavelets are defined by the form:</p>
<p>
<div class='equation' code_block="first">
$$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘”', 'first', 'def', false, '')", "id":"first-ğ‘”", "sym":"ğ‘”", "func":"first",  "localFunc":"", "type":"def", "case":"equation"} }{ {\mathit{ğ‘”}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'x', 'first', 'use', true, 'ğ‘”')", "id":"first-x", "sym":"x", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{x}} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$x_0$', 'first', 'use', true, 'ğ‘”')", "id":"first-$x_0$", "sym":"$x_0$", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {x_0} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœƒ', 'first', 'use', true, 'ğ‘”')", "id":"first-ğœƒ", "sym":"ğœƒ", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{ğœƒ}} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', true, 'ğ‘”')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğ‘”')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) & = exp\left( -\frac{\left\|\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'x', 'first', 'use', true, 'ğ‘”')", "id":"first-x", "sym":"x", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{x}} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$x_0$', 'first', 'use', true, 'ğ‘”')", "id":"first-$x_0$", "sym":"$x_0$", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {x_0} }\right\|_2^{2}}{2{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', true, 'ğ‘”')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }}^{2}} \right)cos\left( 2\pi\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğ‘”')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'x', 'first', 'use', true, 'ğ‘”')", "id":"first-x", "sym":"x", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{x}} } \cdot \begin{bmatrix}
cos\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœƒ', 'first', 'use', true, 'ğ‘”')", "id":"first-ğœƒ", "sym":"ğœƒ", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{ğœƒ}} } \right) & sin\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœƒ', 'first', 'use', true, 'ğ‘”')", "id":"first-ğœƒ", "sym":"ğœƒ", "func":"first",  "localFunc":"ğ‘”", "type":"use", "case":"equation"} }{ {\mathit{ğœƒ}} } \right)\\
\end{bmatrix} \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['x', '$ğ‘“_ğ‘ $', 'ğœƒ', 'ğœ', '$x_0$', 'ğ‘”'], true, 'ğ‘”', ['x', '$x_0$', 'ğœƒ', 'ğœ', '$ğ‘“_ğ‘ $']);"} }{}
\end{align*}
\tag{1}\label{1}$$</div>
</p>
<p>where x denotes the spatial location on the display, x0 is the cen- ter of the wavelet, ğœ is the standard deviation of the Gaussian in visual degrees, and ğ‘“ğ‘  and ğœƒ are the spatial frequency in cpd and angular orientation in degrees for the sinusoidal grating function.</p>
<p>We provide details of the conversion between pixels and eccentric- ity ğ‘’ in the Supplement. Of note however, is that we use a scaled and shifted version of this form to be suitable for display, namely 0.5 + 0.5 ğ‘”(x, x0, ğœƒ, ğœ, ğ‘“ğ‘  ), such that the pattern modulates between 0 and 1, with an average gray level of 0.5. The resulting stimulus exhibits three clearly visible peaks and smoothly blends into the uni- form gray field which covers the entire field of view of our display and ends sharply at its edge with a dark background.</p>
<p>This choice of Gabor wavelet was motivated by many previous works in vision science, including the standard measurement pro- cedure for the CSF [Pelli and Bex 2013] (see Sec. 2), and image processing [Kamarainen et al. 2006; Weldon et al. 1996], where Ga- bor functions are frequently used for their resemblance to neural activations in human vision. For example, it has been shown that 2D Gabor functions are appropriate models of the transfer function of simple cells in the visual cortex of mammalian brains and thus mimicking the early layers of human visual perception [Daugman 1985; MarcË†elja 1980].</p>
<p>As a tradeoff between sampling the parameter space as densely as possible while keeping our user studies to a reasonable length, we converged on using 18 unique test stimuli, as listed in Table 2. We sampled eccentricities ranging from 0â—¦ to almost 60â—¦, moving the fixation point into the nasal direction with increasing eccentricity, such that the target stimulus is affected as little as possible by the lens distortion. We chose not to utilize the full 80â—¦ horizontal FOV of our display due to lens distortion becoming too severe in the last 10â—¦. We chose to test 6 different spatial frequencies, with the highest being limited to 2 cpd due to the lack of a commercial display with both high enough spatial and temporal resolution. However it should be noted that the acuity of human vision is considerably higher; 60 cpd based on peak cone density [Deering 1998] and 40â€“50 cpd based on empirical data [Guenter et al. 2012; Robson and Graham 1981; Thibos et al. 1987]. The Gaussian windows limiting the extent of the Gabor wavelets are scaled according to spatial frequency, i.e., ğœ = 0.7/ğ‘“ğ‘  such that each stimulus exhibits the same number of periods, defining the 6 wavelet orders. Finally, eccentricity values were chosen based on the radius of the wavelet order to uniformly sample the available eccentricity range, as illustrated in Figure 2(b).</p>
<p>The wavelets were temporally modulated by sinusoidally varying the contrast from [âˆ’1, 1] and added to the background gray level of 0.5. In this way, at high temporal frequencies the Gabor wavelet would appear to fade into the background. The control stimulus was modulated at 180 Hz, which is far above the CFF observed for all of the conditions.</p>
<h2 id='user-study'>3.2&nbsp;User Study</h2><p>Participants. Nine adults participated (age range 18â€“53, 4 female). Due to the demanding nature of our psychophysical experiment, only a few subjects were recruited, which is common for similar low-level psychophysics (see e.g. Patney et al. [2016]). Furthermore, previous work measuring the CFF of 103 subjects found a low vari- ance [Romero-GÃ³mez et al. 2007], suggesting sufficiency of a small sample size. All subjects in this and the subsequent experiment had normal or corrected-to-normal vision, no history of visual deficiency, and no color blindness, but were not tested for peripheral-specific abnormalities. All subjects gave informed consent. The research protocol was approved by the Institutional Review Board at the host institution.</p>
<p>Procedure. To start the session, each subject was instructed to position their chin on the headrest such that several concentric circles centered on the right side of the display were minimally distorted (due to the lenses). The threshold for each Gabor wavelet was then estimated in a random order with a two-alternative forced- choice (2AFC) adaptive staircase designed using QUEST [Watson and Pelli 1983]. The orientation of each Gabor patch was chosen randomly at the beginning of each staircase from 0â—¦, 45â—¦, 90â—¦ and 135â—¦. At each step, the subject was shown a small (1â—¦) white cross for 1.5 s to indicate where they should fixate, followed by the test and control stimuli in a random order, each for 1 s. For stimuli at 0â—¦ eccentricity, the fixation cross was removed after the initial display so as not to interfere with the pattern. The screen was momentarily blanked to a slightly darker gray level than the gray background to indicate stimuli switching. The subject was then asked to use a keyboard to indicate which of the two randomly ordered pattens (1 or 2) exhibited more flicker. The ability to replay any trial was also given via key press and the subjects were encouraged to take breaks at their convenience. Each of the 18 stimuli were tested once per user, taking approximately 90 minutes to complete.</p>
<p>Results. Mean CFF thresholds across subjects along with the stan- dard error (vertical bars) and extent of the corresponding stimulus (horizontal bars) are shown in Figure 3. The table of measured values is available in the Supplemental Material.</p>
<p>The measured CFF values have a maximum above 90 Hz. This relatively large magnitude can be explained by the Ferryâ€“Porter law [Tyler and Hamer 1990] and the high adaptation luminance of our display. Similarly large values have previously been observed in corresponding conditions [Tyler and Hamer 1993].</p>
<p>As expected, the CFF reaches its maximum for the lowest ğ‘“ğ‘  values. This trend follows the Granitâ€“Harper law predicting a linear increase of CFF with stimuli area [Hartmann et al. 1979].</p>
<p>For higher ğ‘“ğ‘  stimuli, we observe an increase of the CFF from the fovea towards a peak between 10â—¦ and 30â—¦ of eccentricity. Similar trends have been observed by Hartmann et al. [1979], including the apparent shift of the peak position towards fovea with increasing ğ‘“ğ‘  and decreasing stimuli size.</p>
<p>Finally, our subjects had difficulty to detect flicker for the two largest eccentricity levels for the maximum ğ‘“ğ‘  = 2 cpd. This is predictable as acuity drops close to or below this value for such extreme retinal displacements [Geisler and Perry 1998].</p>
<h1 id='an-eccentricity-dependent-perceptual-model-for-spatio-temporal-flicker-fusion'>4&nbsp;AN ECCENTRICITY-DEPENDENT PERCEPTUAL MODEL FOR SPATIO-TEMPORAL FLICKER FUSION</h1><p>The measured CFFs establish an envelope of spatio-temporal flicker fusion thresholds at discretely sampled points within the resolution afforded by our display prototype. Practical applications, however, require these thresholds to be predicted continuously for arbitrary spatial frequencies and eccentricities. To this end, we develop a continuous eccentricity-dependent model for spatio-temporal flicker fusion that is fitted to our data. Moreover, we extrapolate this model to include spatial frequencies that are higher than those supported by our display by incorporating existing visual acuity data and we account for variable luminance adaptation levels by adapting the Ferryâ€“Porter law [Tyler and Hamer 1993].</p>
<h2 id='model-fitting'>4.1&nbsp;Model Fitting</h2><p>Each of our measured data points is parametrized by its spatial fre- quency ğ‘“ğ‘  , eccentricity ğ‘’ , and CFF value averaged over all subjects. Furthermore, it is associated with a localization uncertainty deter- mined by the radius of its stimulus ğ‘¢. In our design, ğ‘¢ is a function of ğ‘“ğ‘  and, for 13.5% peak contrast cut-off, we define ğ‘¢ = 2ğœ where ğœ = 0.7/ğ‘“ğ‘  to be the standard deviation of our Gabor patches.</p>
<p>We formulate our model as</p>
<p>
<div class='equation' code_block="first">
$$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'Î¨', 'first', 'def', false, 'ğ‘”')", "id":"first-Î¨", "sym":"Î¨", "func":"first",  "localFunc":"ğ‘”", "type":"def", "case":"equation"} }{ {\mathit{Î¨}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'Î¨')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'm', 'first', 'use', false, 'Î¨')", "id":"first-m", "sym":"m", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{m}} }\left( 0,\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 0 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 1 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 2 } + \left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 3 }{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right)}^{2} + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 4 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 5 } \right) \cdot \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right)\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'Î¨')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} } + \left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 6 }{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right)}^{2} + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 7 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'Î¨')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 8 } \right) \cdot \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'Î¨')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right){\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'Î¨')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"Î¨", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} }}^{2} \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['ğ‘â‚ƒ', 'ğ‘â‚ˆ', 'ğœ', 'ğ‘â‚†', 'ğœ', 'ğ‘â‚…', 'm', 'ğ‘â‚‡', 'ğ‘’', '$ğ‘“_ğ‘ $', 'ğ‘â‚‚', 'ğ‘â‚', 'ğ‘â‚€', 'ğ‘â‚„', 'Î¨'], true, 'Î¨', ['ğ‘’', '$ğ‘“_ğ‘ $']);"} }{}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'def', false, 'Î¨')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"Î¨", "type":"def", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğœ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) & = exp\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘', 'first', 'use', false, 'ğœ')", "id":"first-ğ‘", "sym":"ğ‘", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {\mathit{ğ‘}} }_{ 9 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'ğœ')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğœ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) \right) - 1\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['ğ‘â‚‰', 'ğœ', '$ğ‘“_ğ‘ $', 'ğœ'], true, 'ğœ', ['$ğ‘“_ğ‘ $']);"} }{}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'def', false, 'ğœ')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"ğœ", "type":"def", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğœ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'm', 'first', 'use', false, 'ğœ')", "id":"first-m", "sym":"m", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {\mathit{m}} }\left(  \log{ \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğœ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } } -  \log{ \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_{ğ‘ 0}$', 'first', 'use', false, 'ğœ')", "id":"first-$ğ‘“_{ğ‘ 0}$", "sym":"$ğ‘“_{ğ‘ 0}$", "func":"first",  "localFunc":"ğœ", "type":"use", "case":"equation"} }{ {ğ‘“_{ğ‘ 0}} } },0 \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['m', '$ğ‘“_ğ‘ $', '$ğ‘“_{ğ‘ 0}$', 'ğœ'], true, 'ğœ', ['$ğ‘“_ğ‘ $']);"} }{}
\end{align*}
\tag{2}\label{2}$$</div>
<br />
where p = [ğ‘0, . . . , ğ‘9] âˆˆ R10 are the model parameters (see Table 3), ğœ ( ğ‘“ğ‘  ) restricts eccentricity effects for small ğ‘“ğ‘  and ğœ ( ğ‘“ğ‘  ) offsets logarithmic ğ‘“ğ‘  relative to our constant function cut-off.</p>
<p>We build on three domain-specific observations to find a contin- uous CFF model Î¨(ğ‘’, ğ‘“ğ‘  ) : R2 â†’ R that fits our measurements.</p>
<p>First, both our measurements and prior work indicate that the peak CFF is located in periphery, typically between 20â—¦ and 50â—¦ of eccentricity [Hartmann et al. 1979; Rovamo and Raninen 1984;Tyler 1987]. For both fovea and far periphery the CFF drops again forming a convex shape which we model as a quadratic function of ğ‘’.</p>
<p>Second, because the stimuli with very low ğ‘“ğ‘  are not spatially localized, their CFF does not vary with ğ‘’. Consequently, we enforce the dependency on ğ‘’ to converge to a constant function for any ğ‘“ğ‘  belowğ‘“ğ‘ 0 =0.0055cpd.Thiscorrespondstohalfreciprocalofthefull- screen stimuli visual field coverage given our display dimensions.</p>
<p>Finally, following common practices in modeling the effect of spatial frequencies on visual effects, such as contrast [Koenderink et al. 1978c] or disparity sensitivities [Bradshaw and Rogers 1999], we fit the model for logarithmic ğ‘“ğ‘  .</p>
<p>Before parameter optimization, we need to consider the effect of eccentricity uncertainty. The subjects in our study detected flicker regardless of its location within the stimuli extent m = [ğ‘’ Â± ğ‘¢] deg. Therefore, Î¨ achieves its maximum within m and is upper-bounded by our measured flicker frequency ğ‘“ğ‘¡ âˆˆ R. At the same time, nothing can be claimed about the variation of Î¨ within m and therefore, in absence of further evidence, a conservative model has to assume that Î¨ is not lower than ğ‘“ğ‘¡ within m. These two considerations delimit a piece-wise constant Î¨. In practice, based on previous work [Hartmann et al. 1979; Tyler 1987] it is reasonable to assume that Î¨ follows a smooth trend over the retina and its value is lower than ğ‘“ğ‘¡ value at almost all eccentricities within m.</p>
<p>Consequently, in Table 3 we provide two different fits for the parameters. Our conservative model strictly follows the restrictions from the measurement and tends to overestimate the range of visible flicker frequencies which prevents discarding potentially visible signal. Alternatively, our relaxed model follows the smoothness assumption and applies the measured values as upper bound.</p>
<p>To fit the parameters, we used the Adam solver in PyTorch ini- tialized by the Levenbergâ€“Marquardt algorithm and we minimized the mean-square prediction error over all extents m. The additional constraints were implemented as soft linear penalties. To leverage data points with immeasurable CFF values, we additionally force Î¨(ğ‘’, ğ‘“ğ‘  ) = 0 at these points. This encodes imperceptibility of their flicker at any temporal frequency.</p>
<p>Figure 3 shows that the fitted Î¨ represents the expected effects well. The eccentricity curves (row 2) flatten for low ğ‘“ğ‘  and their peaks shift to lower ğ‘’ for large ğ‘“ğ‘  . The conservative fit generally yields larger CFF predictions though it does not strictly adhere to the stimuli extents due to other constraints.</p>
<h2 id='extension-for-high-spatial-frequencies'>4.2&nbsp;Extension for High Spatial Frequencies</h2><p>Due to technical constraints, the highest ğ‘“ğ‘  measured was 2 cpd. At the same time, the acuity of human vision has an upper limit of 60 cpd based on peak cone density [Deering 1998] and 40â€“50 cpd based on empirical data [Guenter et al. 2012; Robson and Graham 1981; Thibos et al. 1987]. To minimize this gap and to generalize our model to other display designs we extrapolate the CFF at higher spatial frequencies using existing models of spatial acuity.</p>
<p>For this purpose, we utilize the acuity model of Geisler and Perry [1998]. It predicts acuity limit ğ´ for ğ‘’ as</p>
<p>
<div class='equation' code_block="first">
$$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'A', 'first', 'def', false, 'ğœ')", "id":"first-A", "sym":"A", "func":"first",  "localFunc":"ğœ", "type":"def", "case":"equation"} }{ {\mathit{A}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'A')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"A", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} } \right) & = \frac{ \ln{ 64 }2.3}{0.106 \cdot \left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'A')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"A", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} } + 2.3 \right)}\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['ğ‘’', 'A'], true, 'A', ['ğ‘’']);"} }{}
\end{align*}
\tag{3}\label{3}$$</div>
</p>
<p>with parameters fitted to measurements of Robson and Graham [1981]. Their study of pattern detection rather than resolution is well aligned with our own study design and conservative visual performance as- sessment. Similarly, their bright adaptation luminance of 500 cd/m2 is also close to our display.</p>
<p>ğ´(ğ‘’) predicts limit of spatial perception. We reason that at this absolute limit flicker is not detectable and, therefore, the CFF is not defined. We represent this situation by zero CFF values in the same way as for imperceptible stimuli in our study and force our model to satisfy Î¨(ğ‘’, ğ´(ğ‘’)) = 0.</p>
<p>In combination with our relaxed constraints we obtain our final full model as shown in Figure 3. It follows the same trends as our original model within the bounds of our measurement space and intersects the zero plane at the projection of ğ´(ğ‘’).</p>
<h2 id='adaptation-luminance'>4.3&nbsp;Adaptation luminance</h2><p>Our experiments were conducted at half of our display peak lu- minance ğ¿ = 380 cd/m2. This is relatively bright compared to the 50â€“200 cd/m2 luminance setting of common VR systems [Mehrfard et al. 2019]. Consequently, our estimates of the CFF are conservative because the Ferryâ€“Porter law predicts the CFF to increase linearly with logarithmic levels of retinal illuminance [Tyler and Hamer 1993]. While the linear relationship is known, the actual slope and intercept varies with retinal eccentricity [Tyler and Hamer 1993]. For this reason, we measured selected points from our main experi- ment for two other display luminance levels.</p>
<p>Four of the subjects from experiment 1 performed the same pro- cedure for a subset of conditions with a display modified first with one and then two additional ND8 filters yielding effective luminance values of 23.9 and 3.0 cd/m2. We then applied a formula of Stan- ley and Davies [1995] to compute the pupil diameter under these conditions as</p>
<p>
<div class='equation' code_block="first">
$$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'd', 'first', 'def', false, 'A')", "id":"first-d", "sym":"d", "func":"first",  "localFunc":"A", "type":"def", "case":"equation"} }{ {\mathit{d}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'L', 'first', 'use', true, 'd')", "id":"first-L", "sym":"L", "func":"first",  "localFunc":"d", "type":"use", "case":"equation"} }{ {\mathit{L}} } \right) & = 7.75 - 5.75\left( \frac{{\left( \frac{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'L', 'first', 'use', true, 'd')", "id":"first-L", "sym":"L", "func":"first",  "localFunc":"d", "type":"use", "case":"equation"} }{ {\mathit{L}} }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'a', 'first', 'use', false, 'd')", "id":"first-a", "sym":"a", "func":"first",  "localFunc":"d", "type":"use", "case":"equation"} }{ {\mathit{a}} }}{846} \right)}^{0.41}}{{\left( \frac{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'L', 'first', 'use', true, 'd')", "id":"first-L", "sym":"L", "func":"first",  "localFunc":"d", "type":"use", "case":"equation"} }{ {\mathit{L}} }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'a', 'first', 'use', false, 'd')", "id":"first-a", "sym":"a", "func":"first",  "localFunc":"d", "type":"use", "case":"equation"} }{ {\mathit{a}} }}{846} \right)}^{0.41} + 2} \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['a', 'L', 'd'], true, 'd', ['L']);"} }{}
\end{align*}
\tag{4}\label{4}$$</div>
<br />
where ğ‘ = 80 Ã— 87 = 6960 deg2 is the adapting area of our display. This allows us to derive corresponding retinal illuminance levels for our experiments using ğ‘™ (ğ¿) = ğœ‹ğ‘‘ (ğ¿)2/4 Â· ğ¿ as 67.3, 321 and 1488 Td and obtain a linear transformation of our original model Î¨(ğ‘’, ğ‘“ğ‘  ) to account for ğ¿ with an eccentricity-dependent slope as</p>
<p>
<div class='equation' code_block="first">
$$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘ ', 'first', 'def', false, 'd')", "id":"first-ğ‘ ", "sym":"ğ‘ ", "func":"first",  "localFunc":"d", "type":"def", "case":"equation"} }{ {\mathit{ğ‘ }} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'ğ‘ ')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} },\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğ‘ ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right) & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğœ', 'first', 'use', false, 'ğ‘ ')", "id":"first-ğœ", "sym":"ğœ", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{ğœ}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$ğ‘“_ğ‘ $', 'first', 'use', true, 'ğ‘ ')", "id":"first-$ğ‘“_ğ‘ $", "sym":"$ğ‘“_ğ‘ $", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {ğ‘“_ğ‘ } } \right)\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'q', 'first', 'use', false, 'ğ‘ ')", "id":"first-q", "sym":"q", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{q}} }_{ 0 }{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'ğ‘ ')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} }}^{2} + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'q', 'first', 'use', false, 'ğ‘ ')", "id":"first-q", "sym":"q", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{q}} }_{ 1 }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'ğ‘’', 'first', 'use', true, 'ğ‘ ')", "id":"first-ğ‘’", "sym":"ğ‘’", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{ğ‘’}} } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'q', 'first', 'use', false, 'ğ‘ ')", "id":"first-q", "sym":"q", "func":"first",  "localFunc":"ğ‘ ", "type":"use", "case":"equation"} }{ {\mathit{q}} }_{ 2 }\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'first', ['ğœ', 'ğ‘’', '$ğ‘“_ğ‘ $', 'q_0', 'q_2', 'q_1', 'ğ‘ '], true, 'ğ‘ ', ['ğ‘’', '$ğ‘“_ğ‘ $']);"} }{}
\end{align*}
\tag{5}\label{5}$$</div>
</p>
<p>where ğ‘™0 = 1488 Td is our reference retinal illuminance, ğœ(ğ‘“ğ‘ ) encodes localization uncertainty for low ğ‘“ğ‘  as in Equation 3 and q = [5.71 Â· 10âˆ’6, âˆ’1.78 Â· 10âˆ’4, 0.204] are parameters obtained by a fit with our full model.</p>
<p>Figure 4 shows that this eccentricity-driven model of Ferryâ€“Porter luminance scaling models not only the slope variation over the retina but also the sensitivity difference over range of ğ‘“ğ‘  well (degree-of- freedom-adjusted R2 = 0.950).</p>
<h1 id='experimental-validation'>5&nbsp;EXPERIMENTAL VALIDATION</h1><p>Our eccentricity-dependent spatio-temporal model is unique in that it allows us to predict what temporal information may be impercep- tible for a certain eccentricity and spatial frequency. One possible application for such a model is in the development of new perceptual video quality assessment metrics (VQMs). Used to guide the devel- opment of different video codecs, encoders, encoding settings, or transmission variants, such metrics aim to predict subjective video quality as experienced by users. While it is commonly known that many existing metrics, such as peak signal-to-noise ratio (PSNR) and structural similarity index metric (SSIM), do not capture many eccentricity-dependent spatio-temporal aspects of human vision well, in this section we discuss a user study we conducted which shows that our model could help better differentiate perceivable and non-perceivable spatio-temporal artifacts. Furthermore, we also use the study to test the fit derived in the previous section, showing that our model makes valid predictions for different users and points beyond those measured in our first user study.</p>
<p>The study was conducted using our custom high-speed VR dis- play with 18 participants, 13 of which did not participate in the previous user study. Users were presented with videos, made up of a single image frame perturbed by Gabor wavelet(s) such that when modulated at a frequency above the CFF they become indistinguish- able from the background image. Twenty-two unique videos were tested, where the user was asked to rank the quality of the video from 1 (â€œbadâ€) to 5 (â€œexcellentâ€). We then calculate the difference mean opinion score (DMOS) per stimulus, as described in detail by Seshadrinathan et al [2010], and 3 VQMs, namely PSNR, SSIM and one of the most influential metrics used today for traditional contents: the Video Multimethod Assessment Fusion (VMAF) metric developed by Netflix [Li et al. 2016]. The results of which are shown in Figure 5. We acknowledge that neither of these metrics has been designed to specifically capture the effects we measure. This often results in uniform scores across the stimuli range (see e.g. Figure 5 III). We include them into our comparison to illustrate these limits and the need for future research in this direction.</p>
<p>The last row shows a comparison of all measured DMOS values with each of the standard VQMs and our own flicker quality predic- tor. This is a simple binary metric that assigns videos outside of the CFF volume where flicker is invisible a good label while it assigns a bad label to others. Our full Î¨(ğ‘’, ğ‘“ğ‘  ) model was used for this pur- pose. We computed Pearson Linear Correlation Coefficients (PLCC) between DMOS and each of the metrics while taking into account their semantics. Only our method exhibits statistically significant correlation with the user scores (ğ‘Ÿ (20) = 0.988, ğ‘ &lt; 0.001 from a two-tail t-test).</p>
<p>Further, we broke our stimuli into 5 groups to test the ability of our model to predict more granular effects.</p>
<ul>
<li>(1) Unseen stimulus. We select ğ‘’ = 30â—¦ and ğ‘“ğ‘  = 1.00 cpd, a configuration not used to fit the model, and show that users rate the video to be of poorer quality (lower DMOS) when it is modulated at a frequency lower than the predicted CFF (see panel I in Fig. 5).</li>
<li>(2) ğ‘“ğ‘  dependence. We test whether our model captures spatio- temporal dependence. Lowering ğ‘“ğ‘  with fixed radius makes the flicker visible, despite not changing ğ‘“ğ‘¡ (panel II).</li>
<li>(3) ğ‘’ change. By moving the fixation point towards the outer edges of the screen, thereby increasing ğ‘’, we show that our model captures and predicts the higher temporal sensitivity of the mid-periphery. Furthermore, by moving the fixation point in both nasal and temporal directions, we validate the assumed symmetry of the CFF. As a result, users were able to notice flicker that was previously imperceptible in the fovea (panel III).</li>
<li>(4) Direction independence. We confirm our assumption that sen- sitivity is approximately rotationally symmetric around the fovea. A single visible stimulus from Group 1 was also added for the analysis as a control (panel IV).</li>
<li>(5) Mixed stimuli. We show that the model predictions hold up even with concurrent viewing of several stimuli of different sizes, eccentricities and spatial frequencies (panel V).</li>
</ul>
<p>The perturbation applied to the videos in Groups 1â€“3 only varied in modulation frequency or eccentricity with respect to a moving gaze position. In response, the dependency of other metrics on ğ‘“ğ‘¡ is on the level of noise, since changes related to variation of ğ‘“ğ‘¡ or ğ‘“ğ‘  of a fixed-size wavelet average out over several frames, and moving the gaze position does not change the frames at all. Our metric is able to capture the perceptual effect, maintaining significant correlation with DMOS (ğ‘Ÿ(3) = 0.994, ğ‘ = 0.001, ğ‘Ÿ(2) = 0.983, ğ‘ &lt; 0.05 and ğ‘Ÿ (3) = 0.977, ğ‘ &lt; 0.001 respectively).</p>
<p>Groups 4 and 5 were designed to be accumulating sets of Ga- bor wavelets, but all with parameters that our model predicts as imperceptible. We observe that VQMs drop with increasing total distortion area, while the DMOS scores stay relatively constant and significantly correlated with our metric (ğ‘Ÿ (2) = 1.000, ğ‘ &lt; 0.001 and ğ‘Ÿ (3) = 0.993, ğ‘ &lt; 0.01 respectively). These results indicate that existing compression schemes may be able to utilize more degrees of freedom to achieve higher compression, without seeing a drop in the typical metrics used to track perceived visual quality. The exact list of chosen parameters, along with the CFFs predicted by our model are listed in the supplemental material.</p>
<p>In conclusion, the study shows that our metric predicts visibility of temporal flicker for independently varying spatial frequencies and retinal eccentricities. While existing image and video quality metrics are important for predicting other visual artifacts, our method is a novel addition in this space that significantly improves perceptual validity of quality predictions for temporally varying content. In this way, we also show that our model may enable existing compres- sion schemes to utilize more degrees of freedom to achieve higher compression, without compromising several conventional VQMs.</p>
<h1 id='analyzing-bandwidth-considerations'>6&nbsp;ANALYZING BANDWIDTH CONSIDERATIONS</h1><p>Our eccentricity-dependent spatio-temporal CFF model defines the gamut of visual signals perceivable to human vision. Hence any signal outside of this gamut can be removed to save bandwidth in computation or data transmission. In this section we provide a theoretical analysis of the compression gain factors this model may potentially enable for foveated graphics applications when used to allocate resources such as bandwidth. In practice, developing foveated rendering and compression algorithms holds other nuanced challenges and thus the reported gains represent an upper bound.</p>
<p>To efficiently apply our model, a video signal should be described by a decomposition into spatial frequencies, retinal eccentricities, and temporal frequencies. We consider discrete wavelet decom- positions (DWT) as a suitable candidate because these naturally decompose a signal into eccentricity-localized spatio-temporal fre- quency bands. Additionally, the multitude of filter scales in the hierarchical decomposition closely resembles scale orders in our study stimuli.</p>
<p>For the purpose of this thought experiment, we use a biorthogonal Haar wavelet, which, for a signal of length ğ‘ , results in log2 (ğ‘ ) hierarchical planes of recursively halving lengths. The total number of resulting coefficients is the same as the input size. From there our baseline is retaining the entire original set of coefficients yielding compression gain of 1.</p>
<p>For traditional spatial-only foveation, we process each frame independently and after a 2D DWT we remove coefficients outside of the acuity limit. We compute eccentricity assuming gaze in the center of the screen and reject coefficients for which Î¨(ğ‘’, ğ‘“ğ‘  ) = 0. From our model definition, such signals cannot be perceived regardless of ğ‘“ğ‘¡ as they lie outside of the vision acuity gamut. The resulting compression gain compared to the baseline can be seen for various display configurations in Figure 6.</p>
<p>Next, for our model we follow the same procedure but we addi- tionally decompose each coefficient of the spatial DWT using 1D temporal DWT. This yields an additional set of temporal coefficients ğ‘“ğ‘¡ and we discard all values with Î¨(ğ‘’, ğ‘“ğ‘  ) &lt; ğ‘“ğ‘¡ .</p>
<p>For this experiment, we assume a screen with the same peak luminance of 380 cd/m2 as our display prototype and a pixel density of 60 pixel per degree (peak ğ‘“ğ‘  = 30 cpd) for approximately retinal display or a fifth of that for a display closer to existing VR systems. The tested aspect ratio of 165:135 approximates the human binocular visual field [Ruch and Fulton 1960]. We consider displays with a maximum framerate of 100 Hz (peak reproducible ğ‘“ğ‘¡ = 50 Hz) and 200 Hz, capable of reaching the maximum CFF in our model. The conventional spatial-only foveation algorithm is not affected by this choice.</p>
<p>Figure 6 shows that this significantly improves compression for both small and large field-of-view displays regardless of pixel density. This is because the spatial-only compression needs to retain all temporal coefficients in order to prevent the worst-scenario flicker at ğ‘’ and ğ‘“ğ‘  with highest CFF. Therefore, it only discards signal components in the horizontal ğ‘’ Ã— ğ‘“ğ‘  plane. On the other hand, our scheme allows to also discard signal in the third temporal dimension closely copying the shape of the Î¨ which allows to reduce retained ğ‘“ğ‘¡ for both fovea and far periphery in particular for high ğ‘“ğ‘  coefficients which require the largest bandwidth.</p>
<h1 id='discussion'>7&nbsp;DISCUSSION</h1><p>The experimental data we measure and the models we fit to them further our understanding of human perception and lay the foun- dation of future spatio-temporal foveated graphics techniques. Yet, several important questions remain to be discussed.</p>
<p>Limitations and Future Work. First, our data and model work with CFF, not the CSF. CFF models the temporal thresholds at which a visual stimulus is predicted to become (in)visible. Unlike the CSF, however, this binary value of visible/invisible does not model rel- ative sensitivity. This makes it not straightforward to apply the CFF directly as an error metric, for example to optimize foveated rendering or compression algorithms. Obtaining an eccentricity- dependent model for the spatio-temporal CSF is an important goal for future work, yet it requires a dense sampling of the full three- dimensional space spanned by eccentricity, spatial frequency, and time with user studies. The CFF, on the other hand only requires us to determine a single threshold for the two-dimensional space of eccentricity and spatial frequency. Considering that obtaining all 18 sampling points in our 2D space for a single user already takes about 90 minutes, motivates the development of a more scalable approach to obtaining the required data in the future.</p>
<p>Although we validate the linear trends of luminance-dependent behavior of the CFF predicted by the Ferryâ€“Porter law, it would be desirable to record more users for larger luminance ranges and more densely spaced points in the 2D sampling space. Again, this would require a significantly larger amount of user studies, which seems outside the scope of this work. Similarly, we extrapolate our model for ğ‘“ğ‘  above the 2 cpd limit of our display. Future work may utilize advancements in display technology or additional optics to confirm the model extension. We also map the spatio-temporal thresholds only for monocular viewing conditions. Some studies have suggested that such visual constraint lowers the measured CFF in comparison to binocular viewing [Ali and Amir 1991; Eisen-Enosh et al. 2020]. Further work is required to confirm the significance of such an effect in the context of displays.</p>
<p>It should also be noted that our model assumes a specific fixation point. We did not account for the dynamics of ocular motion, which may be interesting for future explorations. By simplifying temporal perception to be equivalent to the temporal resolution of the visual system we also do not account for effects caused by our pattern sen- sitive system, such as our ability to detect spatio-temporal changes like local movements or deformations. Similarly, we do not model the effects of crowding in the periphery, which has been shown to dominate spatial resolution in pattern recognition [Rosenholtz 2016]. Finally, all of our data is measured for the green color chan- nel of our display and our model assumes rotational invariance as well as orientation independence of the stimulus. A more nuanced model that studies variations in these dimensions may be valuable future work. All these considerations should be taken into account when developing practical compression algorithms, which is a task beyond the scope of this paper.</p>
<p>Finally, we validate that existing error metrics, such as PSNR, SSIM, and VMAF, do not adequately model the temporal aspects of human vision. The correlation between our CFF data and the DMOS of human observers is significantly stronger, motivating error metrics that better model these temporal aspects. Yet, we do not develop such an error metric nor do we propose practical foveated compression or rendering schemes that directly exploit our CFF data. These investigations are directions of future research.</p>
<p>Conclusion. At the convergence of applied vision science, com- puter graphics, and wearable computing systems design, foveated graphics techniques will play an increasingly important role in emerging augmented and virtual reality systems. With our work, we hope to contribute a valuable foundation to this field that helps spur follow-on work exploiting the particular characteristics of human vision we model.</p>
<h1 id='acknowledgments'>8&nbsp;ACKNOWLEDGMENTS</h1><p>B.K. was supported by a Stanford Knight-Hennessy Fellowship. G.W. was supported by an Okawa Research Grant, a Sloan Fellowship, and a PECASE by the ARO. Other funding for the project was provided by NSF (award numbers 1553333 and 1839974). The authors would also like to thank Brian Wandell, Anthony Norcia, and Joyce Farrell for advising on temporal mechanisms of the human visual system, Keith Winstein for expertise used in the development of the validation study, Darryl Krajancich for constructing the apparatus for our custom VR display, and Yifan (Evan) Peng for assisting with the measurement of the display luminance.</p>
<h1 id='references'>9&nbsp;REFERENCES</h1><p id='ref0'>Rachel Albert, Anjul Patney, David Luebke, and Joohwan Kim. 2017. Latency requirements for foveated rendering in virtual reality. ACM Transactions on Applied Perception (TAP) 14, 4 (2017), 1â€“13.</p>
</body>
</html>