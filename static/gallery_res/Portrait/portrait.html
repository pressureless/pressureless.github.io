<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"> 
    <script>
MathJax = {
  loader: {
    load: ["[attrLabel]/attr-label.js"],
    paths: { attrLabel: "../resource" },
  },
  tex: { packages: { "[+]": ["attr-label"] },
   inlineMath: [['$', '$']]
   },
   options: {
    enableAssistiveMml: false
  },
};
    </script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script src="../resource/d3.min.js"></script>
    <script src="../resource/svg.min.js"></script>
    <script type="text/javascript" src='../resource/paper.js'></script>
    <link rel="stylesheet" href="../resource/paper.css">
</head>
<script>
const iheartla_data = JSON.parse('{"equations":[{"name":"portrait", "parameters":[{"sym":"$I_{in}$", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"A", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"B", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"$I_l$", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"$I_s$", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"M", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"$M_{in}$", "type_info":{"type": "scalar"}},{"sym":"G", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"$σ_{c,}$", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_0"}},{"sym":"$w_{c,}$", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_0"}},{"sym":"$\\\\vec{l}_{key}$", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}},{"sym":"$\\\\vec{n}$", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}},{"sym":"x", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_1"}},{"sym":"y", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_1"}},{"sym":"u", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_2"}},{"sym":"v", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_2"}},{"sym":"select", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"$K_σ$", "type_info":{"type": "scalar"}},{"sym":"$j^{′}$", "type_info":{"type": "scalar"}},{"sym":"λ", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_3"}},{"sym":"$I^*$", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"Φ", "type_info":{"type": "sequence", "element":{"type": "function", "params":[{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}], "ret":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}}, "size":"dim_3"}},{"sym":"f", "type_info":{"type": "function", "params":[{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"},{"type": "scalar"}], "ret":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}}}], "definition":[{"sym":"$I_{out}$", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"I", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}},{"sym":"$M_c$", "type_info":{"type": "scalar"}},{"sym":"$\\\\vec{l}_{fill}$", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}},{"sym":"D", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"dim_1", "cols":"dim_2"}},{"sym":"w", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"dim_1", "cols":"dim_2"}},{"sym":"σ", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"dim_2"}},{"sym":"$L_{feat}$", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"$L_{pix}$", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}},{"sym":"L", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}}], "local_func":[{"name":"$L_{feat}$", "parameters":[{"sym":"θ", "type_info":{"type": "scalar"}}]},{"name":"$L_{pix}$", "parameters":[{"sym":"θ", "type_info":{"type": "scalar"}}]},{"name":"L", "parameters":[{"sym":"θ", "type_info":{"type": "scalar"}}]}], "source":"`$I_{out}$` =`$I_{in}$`∘ A+B\\n\\nwhere\\n\\n`$I_{in}$`: ℝ^(p × q)\\nA: ℝ^(p × q)\\nB: ℝ^(p × q)\\n\\nI =`$I_l$`∘ (1_p,q - M)+`$I_s$`∘M\\n\\nwhere\\n\\n`$I_l$`: ℝ^(p × q)\\n`$I_s$`: ℝ^(p × q)\\nM: ℝ^(p × q)\\n\\n`$M_c$` =sum_k `$M_{in}$` G(`$σ_{c,}$`_k)`$w_{c,}$`_k\\n\\nwhere\\n\\n`$M_{in}$`: ℝ\\nG: ℝ -> ℝ\\n`$σ_{c,}$`_k: ℝ\\n`$w_{c,}$`_k: ℝ\\n\\n`$\\\\vec{l}_{fill}$` = 2(`$\\\\vec{l}_{key}$`⋅`$\\\\vec{n}$`)`$\\\\vec{n}$` - `$\\\\vec{l}_{key}$`\\n\\nwhere\\n\\n`$\\\\vec{l}_{key}$`: ℝ^3\\n`$\\\\vec{n}$`: ℝ^3\\n\\nD_i,j = (x_i - u_j)^2 + (y_i - v_j)^2\\n\\nwhere\\n\\nx_i: ℝ\\ny_i: ℝ\\nu_j: ℝ\\nv_j: ℝ\\n\\nw_i,j = exp(-D_i,j/σ_j)/( sum_`$j^{\\\\prime}$` exp(-D_i,`$j^{\\\\prime}$`/σ_`$j^{\\\\prime}$`))\\n\\n\\nσ_j = select((u_j - u_`$j^{′}$`)^2+(v_j - v_`$j^{′}$`)^2, `$K_σ$`)\\n\\nwhere\\n\\nselect: ℝ, ℝ -> ℝ\\n`$K_σ$`: ℝ\\n`$j^{′}$`: ℝ\\n\\n`$L_{feat}$`(θ) = sum_d λ_d||Φ_d(`$I^*$`) - Φ_d(f(`$I_{in}$`;θ))|| where θ: ℝ\\n`$L_{pix}$`(θ) = ||`$I^*$` - f(`$I_{in}$`;θ)|| where θ: ℝ\\nL(θ) = 0.01 `$L_{feat}$`(θ) + `$L_{pix}$`(θ) where θ: ℝ\\nwhere\\n\\nλ_d: ℝ\\n`$I^*$`: ℝ^(p × q)\\nΦ_d: ℝ^(p × q) -> ℝ^(p × q)\\nf: ℝ^(p × q), ℝ -> ℝ^(p × q)\\n" }] }');
const sym_data = JSON.parse('{"$I_{in}$":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"A":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"B":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$I_l$":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$I_s$":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"M":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$M_{in}$":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"G":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$σ_{c,}$":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_0"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$w_{c,}$":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_0"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$\\\\vec{l}_{key}$":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$\\\\vec{n}$":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"x":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_1"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"y":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_1"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"u":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_2"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"v":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_2"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"select":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"},{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$K_σ$":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$j^{′}$":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"λ":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "scalar"}, "size":"dim_3"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$I^*$":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"Φ":[{"desc":"None", "type_info":{"type": "sequence", "element":{"type": "function", "params":[{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}], "ret":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}}, "size":"dim_3"}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"f":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"},{"type": "scalar"}], "ret":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}}, "def_module":"portrait", "is_defined":false, "used_equations":[]}],"$I_{out}$":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"I":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"p", "cols":"q"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"$M_c$":[{"desc":"None", "type_info":{"type": "scalar"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"$\\\\vec{l}_{fill}$":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"3"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"D":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"dim_1", "cols":"dim_2"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"w":[{"desc":"None", "type_info":{"type": "matrix", "element":{"type": "scalar"}, "rows":"dim_1", "cols":"dim_2"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"σ":[{"desc":"None", "type_info":{"type": "vector", "element":{"type": "scalar"}, "rows":"dim_2"}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"$L_{feat}$":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"$L_{pix}$":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"portrait", "is_defined":true, "used_equations":[]}],"L":[{"desc":"None", "type_info":{"type": "function", "params":[{"type": "scalar"}], "ret":{"type": "scalar"}}, "def_module":"portrait", "is_defined":true, "used_equations":[]}]}');
window.onload = onLoad;
function reportWindowSize() {
  var arrows = document.querySelectorAll(".arrow");
  if (arrows) {
    for (var i = arrows.length - 1; i >= 0; i--) {
      var arrow = arrows[i];
      var body = document.querySelector("body");
      var style = window.getComputedStyle(body);
      var curOffset = parseInt(style.marginLeft, 10)
      var oldOffset = arrow.getAttribute('offset');
      arrow.setAttribute('offset', curOffset);
      // console.log(`oldOffset:${oldOffset}, curOffset:${curOffset}`);
      var arrowStyle = window.getComputedStyle(arrow); 
      var arrowOffset = parseInt(document.querySelector(".arrow").style.marginLeft, 10)
      arrow.style.marginLeft = `${arrowOffset+curOffset-oldOffset}px`;
      var newWidth = parseInt(style.width, 10) + parseInt(style.marginLeft, 10) + parseInt(style.marginRight, 10);
      arrow.style.width = `${newWidth}px`;
      arrow.style.height = style.height; 
      // console.log(`arrow.style.width:${arrow.style.width}, arrow.style.height:${arrow.style.height}`)
    }
  }
  adjsutGlossaryBtn();
}
window.onresize = reportWindowSize;
document.addEventListener("click", function(evt){
    resetState();
});

</script>
<body>
<img src="../resource/glossary.png" id="glossary" class="glossary" alt="glossary" width="22" height="28"><br>
<div class='title'>Portrait Shadow Manipulation</div><div class='author'>XUANER (CECILIA) ZHANG, University of California, Berkeley</div><div class='author'>JONATHAN T. BARRON, Google Research</div><div class='author'>YUN-TA TSAI, Google Research</div><div class='author'>ROHIT PANDEY, Google</div><div class='author'>XIUMING ZHANG, MIT</div><div class='author'>REN NG, University of California, Berkeley</div><div class='author'>DAVID E. JACOBS, Google Research</div><p class='abstract'>Casually-taken portrait photographs often suffer from unflattering lighting and shadowing because of suboptimal conditions in the environment. Aesthetic qualities such as the position and softness of shadows and the lighting ratio between the bright and dark parts of the face are frequently determined by the constraints of the environment rather than by the photographer. Professionals address this issue by adding light shaping tools such as scrims, bounce cards, and flashes. In this paper, we present a computational approach that gives casual photographers some of this control, thereby allowing poorly-lit portraits to be relit post-capture in a realistic and easily-controllable way. Our approach relies on a pair of neural networks—one to remove foreign shadows cast by external objects, and another to soften facial shadows cast by the features of the subject and to add a synthetic fill light to improve the lighting ratio. To train our first network we construct a dataset of real-world portraits wherein synthetic foreign shadows are rendered onto the face, and we show that our network learns to remove those unwanted shadows. To train our second network we use a dataset of Light Stage scans of human subjects to construct input/output pairs of input images harshly lit by a small light source, and variably softened and fill-lit output images of each face. We propose a way to explicitly encode facial symmetry and show that our dataset and training procedure enable the model to generalize to images taken in the wild. Together, these networks enable the realistic and aesthetically pleasing enhancement of shadows and lights in real-world portrait images.</p><ul><li><a href='#introduction'>1&nbsp;INTRODUCTION</a></li><li><a href='#related-work'>2&nbsp;RELATED WORK</a></li><li><a href='#data-synthesis'>3&nbsp;DATA SYNTHESIS</a><ul><li><a href='#foreign-shadows'>3.1&nbsp;Foreign Shadows</a></li><li><a href='#facial-shadows'>3.2&nbsp;Facial Shadows</a></li><li><a href='#facial-symmetry'>3.3&nbsp;Facial Symmetry</a></li></ul></li><li><a href='#neural-network-architecture-and-training'>4&nbsp;NEURAL NETWORK ARCHITECTURE AND TRAINING</a></li><li><a href='#experiments'>5&nbsp;EXPERIMENTS</a><ul><li><a href='#evaluation-data'>5.1&nbsp;Evaluation Data</a></li><li><a href='#ablation-study-of-foreign-shadow-synthesis'>5.2&nbsp;Ablation Study of Foreign Shadow Synthesis</a></li><li><a href='#foreign-shadow-removal-quality'>5.3&nbsp;Foreign Shadow Removal Quality</a></li><li><a href='#facial-shadow-softening-quality'>5.4&nbsp;Facial Shadow Softening Quality</a></li><li><a href='#preprocessing-for-portrait-relighting'>5.5&nbsp;Preprocessing for Portrait Relighting</a></li></ul></li><li><a href='#limitations'>6&nbsp;LIMITATIONS</a></li><li><a href='#conclusion'>7&nbsp;CONCLUSION</a></li><li><a href='#acknowledgements'>8&nbsp;ACKNOWLEDGEMENTS</a></li><li><a href='#reference'>9&nbsp;REFERENCE</a></li></ul>
<figure>
<img src="./img/img1.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 1. The results of our portrait enhancement method on real-world portrait photographs. Casual portrait photographs often suffer from undesirable shadows, particularly foreign shadows cast by external objects, and dark facial shadows cast by the face upon itself under harsh illumination. We propose an automated technique for enhancing these poorly-lit portrait photographs by removing unwanted foreign shadows, reducing harsh facial shadows, and adding synthetic fill lights.
</figcaption> 
</figure>
<h1 id='introduction'>1&nbsp;INTRODUCTION</h1><p>The aesthetic qualities of a photograph are largely influenced by the interplay between light, shadow, and the subject. By controlling these scene properties, a photographer can alter the mood of an image, direct the viewer’s attention, or tell a specific story. Varying the position, size, or intensity of light sources in an environment can affect the perceived texture, albedo, and even three-dimensional shape of the subject. This is especially true in portrait photography, as the human visual system is particularly sensitive to subtle changes in the appearance of human faces. For example, soft lighting (e.g. light from a large area light source like an overcast sky) reduces skin texture, which may cause the subject to appear younger. Conversely, harsh lighting (e.g. light from a small or distant source like the midday sun) may exaggerate wrinkles and facial hair, making a subject appear older. Similarly, any shadows falling on the subject’s face can accentuate its three-dimensional structure or obfuscate it with distracting intensity edges that are uncorrelated with facial geometry. Other variables such as the lighting angle (the angle at which light strikes the subject) or the lighting ratio (the ratio of illumination intensity between the brightest and darkest portion of a subject’s face) can affect the dramatic quality of the resulting photograph, or may even affect some perceived quality of the subject’s personality: harsh lighting may look “serious”, or lighting from below may make the subject look “sinister”.</p>
<p>Unfortunately, though illumination is clearly critical to the appearance of a photograph, finding or creating a good lighting environment outside of a studio is challenging. Professional photographers spend significant amounts of time and effort directly modifying the illumination of existing environments through physical means, such as elaborate lighting kits consisting of scrims (cloth diffusers), reflectors, flashes, and bounce cards [<a class="citation" href="#ref-grey2014master" id="cite-grey2014master">Grey 2014</a>].</p>
<p>In this work, we attempt to provide some of the control over lighting that professional photographers have in studio environments to casual photographers in unconstrained environments. We present a framework that allows casual photographers to enhance the quality of light and shadow in portraits from a single image after it has been captured. We target three specific lighting problems common in casual photography and uncontrolled environments:</p>
<p>Foreign shadows: We will refer to any shadow cast on the subject’s face by an external occluder (e.g. a tree, a hat brim, an adjacent subject in a group shot, the camera itself, etc.) as a foreign shadow. Notably, foreign shadows can result in an arbitrary two-dimensional shape in the final photograph, depending on the shape of the occluder and position of the primary, or key, light source. Accordingly, they frequently introduce image intensity edges that are uncorrelated with facial geometry and therefore are almost always distracting. Because most professional photographers would remove the occluder or move the subject in these scenarios, we will address this type of shadow by attempting to remove it entirely.</p>
<p>Facial shadows: We will refer to any shadow cast on the face by the face itself (e.g. the shadow attached to the nose when lit from the side) as a facial shadow. Because facial shadows are generated by the geometry of the subject, these shadows (unlike foreign shadows) can only project to a small space of two-dimensional shapes in the final image. Though they may be aesthetically displeasing, the image intensity edges introduced by facial shadows are more likely than foreign shadows to be a meaningful cue for the shape of the subject. Because facial shadows are almost always present in natural lighting environments (i.e., the environment is not perfectly uniform), we do not attempt to remove them entirely. We instead emulate a photographer’s scrim in this scenario, which effectively increases the size of the key light and softens the edges of the shadows it casts.</p>
<p>Lighting ratios: In scenes with very strong key lights (e.g. outdoors on a clear day), the ratio between the illumination of the brightest and darkest parts of the face may exceed the dynamic range of the camera, resulting in a portrait with dark shadows or blown out highlights. While this can be an intentional artistic choice, typical portrait compositions target less extreme lighting ratios. Professional photographers balance lighting ratios by placing a secondary, or fill, light in the scene opposite the key. We similarly place a virtual fill light to balance the lighting ratio and add definition to the shape of the shadowed portion of the subject’s face.</p>
<p>Our framework consists of two machine learning models: one trained for foreign shadow removal, and another trained for handling facial shadow softening and lighting ratio adjustment. This grouping of tasks is motivated by two related observations.</p>
<p>Our first observation, as mentioned above, is tied to the differing relationships between shadow appearance and facial geometry. The appearance of facial shadows in the input image provides a significant cue for shape estimation, and should therefore be useful input when synthesizing an image with softer facial shadowing and a smaller lighting ratio. But foreign shadows are much less informative, and so we first identify and remove all foreign shadows before attempting to perform facial shadow manipulation. This approach provides our facial shadow model with an image in which all shadow-like image content is due to facial shadows, and also happens to be consistent with contemporary theories on how the human visual system perceives shadows [<a class="citation" href="#ref-rensink2004influence" id="cite-rensink2004influence">Rensink and Cavanagh 2004</a>].</p>
<p>Our second observation relates to training dataset requirements. Thanks to the unconstrained nature of foreign shadow appearance, it is possible to train our first network with a synthetic dataset: 5000 “in-the-wild” images, augmented with randomly generated foreign shadows for a total of 500K training examples. This strategy is not viable for our second network, as facial shadows must be consistent with the geometry of the subject and so cannot be generated in this way. Constructing an “in-the-wild” dataset consisting entirely of images with controlled facial shadowing is also intractable. We therefore synthesize the training data for this task using one-light-at-a-time (OLAT) scans taken by a Light Stage, an acquisition setup and method proposed to capture reflectance field [<a class="citation" href="#ref-debevec2000acquiring" id="cite-debevec2000acquiring">Debevec et al. 2000</a>] of human faces. We use the Light Stage scans to synthesize paired harsh/soft images for use as training data. Section 3 will discuss our dataset generation procedure in more detail.</p>
<p>Though trained separately, the neural networks used for our two tasks share similar architectures: both are deep convolutional networks for which the input is a 256 × 256 resolution RGB image of a subject’s face. The output of each network is a per-pixel and per-channel affine transformation consisting of a scaling A and offset B, at the same resolution as the input image Iin such that the final output Iout can be computed as:<br />

        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_{out}$', 'portrait', 'def', false, '')", "id":"portrait-$I_{out}$", "sym":"$I_{out}$", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {I_{out}} } & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_{in}$', 'portrait', 'use', false, '')", "id":"portrait-$I_{in}$", "sym":"$I_{in}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {I_{in}} } \circ \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'A', 'portrait', 'use', false, '')", "id":"portrait-A", "sym":"A", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{A}} } + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'B', 'portrait', 'use', false, '')", "id":"portrait-B", "sym":"B", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{B}} }\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$I_{in}$', 'B', 'A', '$I_{out}$'], false, []);"} }{}
\end{align*}
\tag{1}\label{1}$$</div>
        </p>
<p>where ◦ denotes per-element multiplication. This approach can be thought of as an extension of quotient images [<a class="citation" href="#ref-shashua2001quotient" id="cite-shashua2001quotient">Shashua and Riklin-Raviv 2001</a>] and of residual skip connections [<a class="citation" href="#ref-he2016deep" id="cite-he2016deep">He et al. 2016</a>], wherein our network is encouraged to produce output images that resemble scaled and shifted versions of the input image. The facial shadow network includes additional inputs that are concatenated onto the input RGB image: 1) two numbers encoding the desired shadow softening and fill light brightness, so that variable amounts of softening and fill lighting can be specified and 2) an additional rendition of the input image with the face mirrored about its axis of symmetry (i.e., pixels corresponding to the left eye of the input are warped to the position of the right eye, and vice versa). Using a mirrored face image in this way broadens the spatial support of the first layer of our network to include the image region on the opposite side of the subject’s face. This allows the network to exploit the bilateral symmetry of human faces and to easily “borrow” pixels with similar semantic meaning and texture but different lighting from the opposite side of the subject’s face (see Section 3.3 for details).</p>
<p>In addition to the framework itself, this work presents the following technical contributions:<br />
- (1) Techniques for generating synthetic, real-world, and Light Stage-based datasets for training and evaluating machine learning models targeting foreign shadows, facial shadows, and virtual fill lights.<br />
- (2) Symmetric face image generation for explicitly encoding symmetry cue into training our facial shadow model.<br />
- (3) Ablation studies that demonstrate our data and models achieve portrait enhancement results that outperform all baseline methods in numerical metrics and perceptual quality.</p>
<p>The remainder of the paper is organized as follows. Section 2 describes prior work in lighting manipulation, shadow removal, and portrait retouching. Section 3 introduces our synthetic dataset generation procedure and our real ground-truth data acquisition. Section 4 talks about our network architecture and training procedure. Section 5 shows a series of ablation studies and presents qualitative and quantitative results and comparisons. Section 6 discusses limitations of our approach.</p>
<h1 id='related-work'>2&nbsp;RELATED WORK</h1><p>The detection and removal of shadows in images is a central problem within computer vision, as is the closely related problem of separating image content into reflectance and shading [<a class="citation" href="#ref-horn1974determining" id="cite-horn1974determining">Horn 1974</a>]. Many graphics-oriented shadow removal solutions rely on manually-labeled “shadowed” or “lit” regions [<a class="citation" href="#ref-arbel2010shadow" id="cite-arbel2010shadow">Arbel and Hel-Or 2010</a>; <a class="citation" href="#ref-gryka2015learning" id="cite-gryka2015learning">Gryka et al. 2015</a>; <a class="citation" href="#ref-shor2008shadow" id="cite-shor2008shadow">Shor and Lischinski 2008</a>; <a class="citation" href="#ref-wu2007natural" id="cite-wu2007natural">Wu et al. 2007</a>]. Once manually identified, shadows can be removed by solving a global optimization technique, such as graph cuts. Because relying on user input limits the applicability of these techniques, fully-automatic shadow detection and manipulation algorithms have also attracted substantial attention. Illumination discontinuity across shadow edges [<a class="citation" href="#ref-sato2003illumination" id="cite-sato2003illumination">Sato et al. 2003</a>] can be used to detect and remove shadows [<a class="citation" href="#ref-baba2003shadow" id="cite-baba2003shadow">Baba and Asada 2003</a>]. Formulating shadow enhancement as local tone adjustment and using edge-preserving histogram manipulation [<a class="citation" href="#ref-kaufman2012content" id="cite-kaufman2012content">Kaufman et al. 2012</a>] enables contrast enhancement on semantically segmented photographs. Relative differences in the material and illumination of paired image segments [<a class="citation" href="#ref-guo2012paired" id="cite-guo2012paired">Guo et al. 2012</a>; <a class="citation" href="#ref-ma2016appearance" id="cite-ma2016appearance">Ma et al. 2016</a>] enables the training of region-based classifiers and the use of graph cuts for labeling and shadow removal. Shadow removal has also been formulated as an entropy minimization problem [<a class="citation" href="#ref-finlayson2009entropy" id="cite-finlayson2009entropy">Finlayson et al. 2009</a>; <a class="citation" href="#ref-finlayson2002removing" id="cite-finlayson2002removing">Finlayson et al. 2002</a>], where invariant chromaticity and intensity images are used to produce a shadow mask that is then re-integrated to form a shadow-free image. These methods assume that shadow regions contain approximately constant reflectance and that image gradients are entirely due to changes in illumination, and are thereby fail when presented with complex spatially-varying textures or soft shadowing. In addition, by decomposing the shadow removal problem into two separate stages of detection and manipulation, these methods cannot recover from errors during the shadow detection step [<a class="citation" href="#ref-ma2016appearance" id="cite-ma2016appearance">Ma et al. 2016</a>].</p>
<p>General techniques for inverse rendering [<a class="citation" href="#ref-ramamoorthi2001signal" id="cite-ramamoorthi2001signal">Ramamoorthi and Hanrahan 2001</a>; <a class="citation" href="#ref-sengupta2018sfsnet" id="cite-sengupta2018sfsnet">Sengupta et al. 2018</a>] and intrinsic image decomposition [<a class="citation" href="#ref-barron2014shape" id="cite-barron2014shape">Barron and Malik 2014</a>; <a class="citation" href="#ref-grosse2009ground" id="cite-grosse2009ground">Grosse et al. 2009</a>] should, in theory, be useful for shadow removal, as they provide shading and reflectance decompositions of the image. However, in practice these techniques perform poorly when used for shadow removal (as opposed to shading removal) and usually consider cast shadows to be out of scope. For example, the canonical Retinex algorithm [<a class="citation" href="#ref-horn1974determining" id="cite-horn1974determining">Horn 1974</a>] assumes that shading variation is smooth and monochromatic and therefore fails catastrophically on simple cases such as shadows cast by the midday sun, which are usually non-smooth and chromatic (sunlit yellow outside the shadow, and sky blue within).</p>
<p>More recently, learning-based approaches have demonstrated a significant improvement on general-purpose shadow detection and manipulation [<a class="citation" href="#ref-cun2020towards" id="cite-cun2020towards">Cun et al. 2020</a>; <a class="citation" href="#ref-ding2019argan" id="cite-ding2019argan">Ding et al. 2019</a>; <a class="citation" href="#ref-hu2019direction" id="cite-hu2019direction">Hu et al. 2019</a>; <a class="citation" href="#ref-hu2018direction" id="cite-hu2018direction">Hu et al. 2018</a>; <a class="citation" href="#ref-khan2015automatic" id="cite-khan2015automatic">Khan et al. 2015</a>; <a class="citation" href="#ref-zheng2019distraction" id="cite-zheng2019distraction">Zheng et al. 2019</a>; <a class="citation" href="#ref-zhu2018bidirectional" id="cite-zhu2018bidirectional">Zhu et al. 2018</a>]. However, like all learned techniques, such approaches are limited by the nature of their training data. While real-world datasets for general shadow removal are available [<a class="citation" href="#ref-qu2017deshadownet" id="cite-qu2017deshadownet">Qu et al. 2017</a>; <a class="citation" href="#ref-wang2018stacked" id="cite-wang2018stacked">Wang et al. 2018</a>], they do not include human subjects and therefore are unlikely to be useful for our task, which requires the network to reason about specific visual characteristics of faces, such as the skin’s subsurface scattering effect [<a class="citation" href="#ref-donner2006spectral" id="cite-donner2006spectral">Donner and Jensen 2006</a>]. Instead, in this paper, we propose to train a model using synthetic shadows generated on images in the wild. We only use images of faces to encourage the model to learn and use priors on human faces. Earlier work has shown that training models on faces improves performance on face-specific subproblems of common tasks, such as inpainting [<a class="citation" href="#ref-ulyanov2018deep" id="cite-ulyanov2018deep">Ulyanov et al. 2018</a>; <a class="citation" href="#ref-yeh2017semantic" id="cite-yeh2017semantic">Yeh et al. 2017</a>], super-resolution [<a class="citation" href="#ref-chen2018fsrnet" id="cite-chen2018fsrnet">Chen et al. 2018</a>] and synthesis [<a class="citation" href="#ref-denton2015deep" id="cite-denton2015deep">Denton et al. 2015</a>].</p>
<p>Another problem related to ours is “portrait relighting”—the task of relighting a single image of a human subject according to some desired environment map [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>; <a class="citation" href="#ref-zhou2019deep" id="cite-zhou2019deep">Zhou et al. 2019</a>]. These techniques could theoretically be used for our task, as manipulating the facial shadows of a subject is equivalent to re-rendering that subject under a modified environmental illumination map in which the key light has been dilated. However, as we will demonstrate (and was noted in [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>]) these techniques struggle when presented with images that contain foreign shadows or high-frequency image structure due to harsh shadows in the input image, which our approach specifically addresses. Example-based portrait lighting transfer techniques [<a class="citation" href="#ref-shih2014style" id="cite-shih2014style">Shih et al. 2014</a>; <a class="citation" href="#ref-shu2017portrait" id="cite-shu2017portrait">Shu et al. 2017</a>] also represent potential alternative solutions to this task, but they require a high-quality reference image that exhibits the desired lighting, and that also contains a subject with a similar identity and pose as the input image—an approach that does not scale to casual photos in the wild.</p>
<h1 id='data-synthesis'>3&nbsp;DATA SYNTHESIS</h1><p>There is no tractable data acquisition method to collect a large-scale dataset of human faces for our task with diversity in both the subject and the shadows, as the capture process would be onerous for both the subjects (who must remain perfectly still for impractically long periods of time) and the photographers (who must be specially trained for the task and find thousands of willing participants in thousands of unique environments). Instead, we synthesize custom datasets for our subproblems by augmenting existing datasets— Recall that our two models require fundamentally different training data. Our foreign shadow datasets (Section 3.1) are based on images of faces in the wild with rendered shadows, while our facial shadow and fill light datasets (Section 3.2) are based on a Light Stage dataset with carefully chosen simulated environments.</p>
<h2 id='foreign-shadows'>3.1&nbsp;Foreign Shadows</h2><p>To synthesize images that appear to contain foreign shadows, we model images as a linear blend between a “lit” image Il and a “shadowed” image Is , according to some shadow mask M :</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'I', 'portrait', 'def', false, '')", "id":"portrait-I", "sym":"I", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {\mathit{I}} } & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_l$', 'portrait', 'use', false, '')", "id":"portrait-$I_l$", "sym":"$I_l$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {I_l} } \circ \left( \mathbb{ 1 }_{ \mathit{p},\mathit{q} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'M', 'portrait', 'use', false, '')", "id":"portrait-M", "sym":"M", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{M}} } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_s$', 'portrait', 'use', false, '')", "id":"portrait-$I_s$", "sym":"$I_s$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {I_s} } \circ \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'M', 'portrait', 'use', false, '')", "id":"portrait-M", "sym":"M", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{M}} }\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$I_l$', 'M', '$I_s$', 'I'], false, []);"} }{}
\end{align*}
\tag{2}\label{2}$$</div>
        </p>
<p>The lit image Il is assumed to contain the subject lit by all light sources in the scene (e.g. the sun and the sky), and the shadowed image Is is assumed to be the subject lit by everything other than the key (e.g. just the sky). The shadow mask M indicates which pixels are shadowed: M = 1 if fully shadowed, and M = 0 if fully lit. To generate a training sample, we need Il, Is, and M. Il is selected from an initial dataset described below, Is is a color transform of Il, and M comes from a silhouette dataset or a pseudorandom noise function.</p>
<p>Because deep learning models are highly sensitive to the realism and biases of the data used during training, we take great care to synthesize as accurate a shadow mask and shadowed image as possible with a series of augmentations on Is and M. Figure 2 presents an overview of the process and below we enumerate different aspects of our synthesis model and their motivation. In Section 5.2, we will demonstrate their efficacy through an ablation study.</p>
<figure>
<img src="./img/img2.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 2. The pipeline of our foreign shadow synthesis model (Section 3.1). The colors of the “lit” image Il are randomly jittered to generate a “shadow” image ${\proselabel{portrait}{{I_s}}}$ . The input mask Min shown here is generated from an object silhouette, though it may also be generated with Perlin noise. Min is subjected to a subsurface scattering (SS) approximation of human skin to generate Mss, then a spatially-varying blur and per-pixel intensity variation to generate M. Il and Is are then blended according to the shadow mask M to generate a training sample I .
</figcaption> 
</figure>
<figure>
<img src="./img/img3.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 3. Our facial shadow synthesis model. Our input image is a OLAT render corresponding to an environment with a single key light turned on as shown in (a). To soften the shadows by some variable amount, we distribute the key’s energy to a variable number of its neighbors, as shown in (b) and (c). We also add a number of fill lights on the opposite side of the Light Stage, to brighten the darker side of the face as shown in (d), with the fill light’s difference image visualized in (e). For clarity, only half of the Light Stage’s lights are rendered. See the supplemental video at 02:04 for a few more examples.
</figcaption> 
</figure>

<p>Input Images: Our dataset is based on a set of 5,000 faces in the wild that we manually identified as not containing any foreign shadows. These images are real, in-the-wild JPEG data, and so they are varied in terms of subject, ethnicity, pose, and environment. Common accessories such as hats and scarves are included, but only if they do not cast shadows. We make one notable exception to this policy: glasses. Shadows from glasses are unavoidable and behave more like facial shadows than foreign. Accordingly, shadows from glasses are preserved in our ground truth. Please refer to the supplement for examples.</p>
<p>Light Color Variation: The shadowed image region Is is illuminated by a lighting environment different from that of the non-shadow region. For example, outdoor shadows are often tinted blue because when the sun is blocked, the blue sky becomes the dominant light source. To account for such illumination differences, we apply a random color jitter, formulated as a 3 × 3 color correction matrix, to the lit image Il. Please see the supplement for details.</p>
<p>Shape Variation: The shapes of natural shadows are as varied as the shapes of natural objects in the world, but those natural shapes also exhibit significant statistical regularities [<a class="citation" href="#ref-huang2000statistics" id="cite-huang2000statistics">Huang et al. 2000</a>]. To capture both the variety and the regularity of real-world shadows, our distribution of input shadow masks Min is half “regular” real-world masks drawn from a dataset of 300 silhouette images of natural objects, randomly scaled and tiled; and half “irregular” masks generated using a Perlin noise function at 4 octaves with a persistence drawn uniformly at random within [0, 0.85], with the initial amplitude set to 1.0.</p>
<p>Subsurface Scattering: Light scatters beneath the surface of human skin before exiting, and the degree of that scattering is wavelength-dependent [<a class="citation" href="#ref-hanrahan1993reflection" id="cite-hanrahan1993reflection">Hanrahan and Krueger 1993</a>; <a class="citation" href="#ref-jensen2001practical" id="cite-jensen2001practical">Jensen et al. 2001</a>; <a class="citation" href="#ref-krishnaswamy2004biophysically" id="cite-krishnaswamy2004biophysically">Krishnaswamy and Baranoski 2004</a>]: blood vessels cause red light to scatter further that other wavelengths, causing a visible color fringe at shadows. We approximate the subsurface scattering appearance by uniformly blurring Min with a different kernel per color channel, borrowing from [<a class="citation" href="#ref-fernando2004gpu" id="cite-fernando2004gpu">Fernando and others 2004</a>]. In brief, the kernel for each channel is a sum of <span sym='σ_{c,}' context='portrait'>  Gaussians $G({ {\prosedeflabel{portrait}{{σ_{c,}}}} }_k )$   </span>with <span sym='w_{c,}' context='portrait'>  weights ${ {\prosedeflabel{portrait}{{w_{c,}}}} }_k$   </span>, such that each channel ${\proselabel{portrait}{{M_c}}}$ of the shadow mask with subsurface scattering $M_{ss}$ is rendered as:</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$M_c$', 'portrait', 'def', false, '')", "id":"portrait-$M_c$", "sym":"$M_c$", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {M_c} } & = \sum_{\mathit{k}} \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$M_{in}$', 'portrait', 'use', false, '')", "id":"portrait-$M_{in}$", "sym":"$M_{in}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {M_{in}} }\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'G', 'portrait', 'use', false, '')", "id":"portrait-G", "sym":"G", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{G}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$σ_{c,}$', 'portrait', 'use', false, '')", "id":"portrait-$σ_{c,}$", "sym":"$σ_{c,}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {σ_{c,}} }_{ \mathit{k} } \right)\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$w_{c,}$', 'portrait', 'use', false, '')", "id":"portrait-$w_{c,}$", "sym":"$w_{c,}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {w_{c,}} }_{ \mathit{k} }\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['G', '$M_{in}$', '$w_{c,}$', '$σ_{c,}$', '$M_c$'], false, []);"} }{}
\end{align*}
\tag{3}\label{3}$$</div>
        </p>
<p>Spatial Variation: The softness of the shadow being cast on a subject depends on the relative distances between the subject, the key light, and the object casting the shadow. Because this relationship varies over the image, our shadow masks incorporate a spatially-varying blur over $M_{ss}$. While many prior works assume that the shadow region has a constant intensity [<a class="citation" href="#ref-zhang2019effective" id="cite-zhang2019effective">Zhang et al. 2019</a>], we note that a partially translucent occluder or an environment violating the assumption that lights are infinitely far away will cause shadows to have different local intensities. Accordingly, we similarly apply a spatially-varying per-pixel intensity variation to $M_{ss}$ as well, modeled as Perlin noise at 2 octaves with a persistence drawn uniformly at random from [0.05, 0.25] and an initial amplitude set to 1.0. The final mask with spatial variation incorporated is what we refer to as M above.</p>
<h2 id='facial-shadows'>3.2&nbsp;Facial Shadows</h2><p>We are not able to use “in-the-wild” images for synthesizing facial shadows because the highly accurate facial geometry it would require is generally not captured in such datasets. Instead, we use Light Stage data that can relight a scanned subject with perfect fidelity under any environment and select the simulated environment with care. Note that we cannot use light stage data to produce more accurate foreign shadows than we could using raw, in-the-wild JPEG images, which is why we adopt different data synthesis for these two tasks.</p>
<p>When considering foreign shadows, we adopt shadow removal with the rationale that foreign shadows are likely undesirable from a photographic perspective and removing them does not affect the apparent authenticity of the photograph (as the occluder is rarely in frame). Facial shadows, in contrast, can only be softened if we wish to affect the mood of the photograph while remaining faithful to the scene’s true lighting direction.</p>
<p>We construct our dataset by emulating the scrims and bounce cards employed by professional photographers. Specifically, we generate harsh/soft facial shadow pairs using OLAT scans from a Light Stage dataset. This is ideal for two reasons: 1) each individual light in the stage is designed to match the angular extent of the sun, so it is capable of generating harsh shadows, and 2) with such a dataset, we can render an image I simulating an arbitrary lighting environment with a simple linear combination of OLAT images Ii withweightswi,i.e.,${\proselabel{portrait}{{I}}} = \sum_i I_i w_i$.</p>
<p>For each training instance, we select one of the 304 lights in the stage and dub it our key light with index $i_{key}$, and use its location to define <span sym='\vec{l}_{key}' context='portrait'>  the key light direction ${\prosedeflabel{portrait}{{\vec{l}_{key}}}}$  </span>. Our harsh input image is defined to be one corresponding to OLAT weights $w_i$ = {$P_{key}$ if $i = i_{key}$, $ε$ otherwise, where $P_{key}$ is a randomly sampled intensity of the key light and $ε$ is a small non-zero value that adds ambient light to prevent shadowed pixels from becoming fully black. The corresponding soft image is then rendered by splatting the key light energy to the set of its $m$ nearest neighboring lights $Ω( {\proselabel{portrait}{{\vec{l}_{key}}}} )$, where $m$ is drawn uniformly from a set of discrete numbers [5, 10, 20, 30, 40]. This can be thought of as convolving the key light source with a disc, similar in spirit to a diffuser or softbox. We then compute the location of the fill light (Figure 3(d)):</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$\\\\vec{l}_{fill}$', 'portrait', 'def', false, '')", "id":"portrait-$\\\\vec{l}_{fill}$", "sym":"$\\\\vec{l}_{fill}$", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {\vec{l}_{fill}} } & = 2\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$\\\\vec{l}_{key}$', 'portrait', 'use', false, '')", "id":"portrait-$\\\\vec{l}_{key}$", "sym":"$\\\\vec{l}_{key}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\vec{l}_{key}} } \cdot \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$\\\\vec{n}$', 'portrait', 'use', false, '')", "id":"portrait-$\\\\vec{n}$", "sym":"$\\\\vec{n}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\vec{n}} } \right)\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$\\\\vec{n}$', 'portrait', 'use', false, '')", "id":"portrait-$\\\\vec{n}$", "sym":"$\\\\vec{n}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\vec{n}} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$\\\\vec{l}_{key}$', 'portrait', 'use', false, '')", "id":"portrait-$\\\\vec{l}_{key}$", "sym":"$\\\\vec{l}_{key}$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\vec{l}_{key}} }\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$\\\\vec{l}_{key}$', '$\\\\vec{n}$', '$\\\\vec{l}_{fill}$'], false, []);"} }{}
\end{align*}
\tag{4}\label{4}$$</div>
        </p>
<p>where <span sym='\vec{n}' context='portrait'>  ${\prosedeflabel{portrait}{{\vec{n}}}}$ is the unit vector along the camera z-axis, pointing out of the Light Stage  </span>. For all data generation, we use a fixed fill light neighborhood size of 20, and a random fill intensity $P_{fill}$ in $[0, P_{key} /10]$. Thus, the soft output image is defined as one corresponding to OLAT weights</p>
<p>$$
w_i = \begin{cases} 
P_{key} / m, &  \text{if } i \in \Omega( {\proselabel{portrait}{{\vec{l}_{key}}}} ) \\ 
P_{fill}, &  \text{if } i \in \Omega( {\proselabel{portrait}{{\vec{l}_{fill}}}}    ) \\ 
ε & \text{otherwise} 
\end{cases} 
\notag$$</p>
<p>To train our facial shadow model, we use OLAT images of 85 subjects, each of which was imaged under different expressions and poses, giving us in total 1795 OLAT scans to render our facial harsh shadow dataset. We remove degenerate lights that cause strong flares or at extreme angles that render too dark images, and end up using the remaining 284 lights for each view.</p>
<h2 id='facial-symmetry'>3.3&nbsp;Facial Symmetry</h2><p>Human faces tend to be bilaterally symmetric: the left side of most faces closely resembles the right side in terms of geometry and reflectance, except for the occasional blemish or minor asymmetry. However, images of faces are rarely symmetric because of facial shadows. Therefore, if a neural network can easily reason about the symmetry of image content on the subject’s face, it will be able to do a better job of reducing shadows cast upon that face. For this reason, we augment the image that is input to our neural networks with a “mirrored” version of that face, thereby giving the early layers of those networks the ability to straightforwardly reason about which image content is present on the opposite side of the face. Because the subject’s face is rarely perfectly vertical and oriented perpendicularly to the camera’s viewing direction, it is not sufficient to simply mirror the input image along the x-axis. We therefore estimate the geometry of the face and mirror the image using that estimated geometry, by warping image content near each vertex of a mesh to the location of its corresponding mirrored vertex. See Figure4foravisualization.</p>
<figure>
<img src="./img/img4.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 4. The symmetry of human faces is a useful cue for reasoning about lighting: a face’s reflectance and geometry is likely symmetric, but the shadow cast upon that face is likely not symmetric. To leverage this, a landmark detection system is applied to the input image (a) and the recovered landmark (b) are used to produce a per-pixel mirrored version of the input image (c). This mirrored image is appended to the input image in our networks, which improves performance by allowing the network to directly reason about asymmetric image content (d) which is likely due to facial and foreign shadows.
</figcaption> 
</figure>
<p>Given an image I , we use the landmark detection system of [<a class="citation" href="#ref-kartynnik2019real" id="cite-kartynnik2019real">Kartynnik et al. 2019</a>] to produce a model of facial geometry consisting of 468 2D vertices (Figure 4(b)) and a mesh topology (which is fixed for all instances). For each vertex $j$ we precompute the index of its bilaterally symmetric vertex $j$, which corresponds to a vertex $(u_{\bar{j}}, v_{\bar{j}})$ at the same position as $(u_j,v_j)$ but on the opposite side of the face. With this correspondence we could simply produce a “mirrored” version of I by applying a meshwarp to I where the position of each vertex j is moved to the position of its mirror vertex j. However, a straightforward meshwarp is prone to triangular-shaped artifacts and irregular behavior on foreshortened triangles or inaccurately-estimated keypoint locations. For this reason we instead use a “soft” warping approach based on an adaptive radial basis function (RBF) kernel: For each pixel in I we compute its RBF weight with respect to the 2D locations of all vertices, express that pixel location as a convex combination of all vertex locations, and then interpolate the “mirrored” pixel location by computing the same convex combination of all mirrored vertex locations. Put formally, we first compute the Euclidean distance from all pixel locations to all vertex locations:</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'D', 'portrait', 'def', false, '')", "id":"portrait-D_ij", "sym":"D", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {\mathit{D}} }_{\mathit{i},\mathit{j}} & = {\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'x', 'portrait', 'use', false, '')", "id":"portrait-x", "sym":"x", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{x}} }_{ \mathit{i} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'u', 'portrait', 'use', false, '')", "id":"portrait-u", "sym":"u", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{u}} }_{ \mathit{j} } \right)}^{2} + {\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'y', 'portrait', 'use', false, '')", "id":"portrait-y", "sym":"y", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{y}} }_{ \mathit{i} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'v', 'portrait', 'use', false, '')", "id":"portrait-v", "sym":"v", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{v}} }_{ \mathit{j} } \right)}^{2}\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['y', 'u', 'x', 'v', 'D'], false, []);"} }{}
\end{align*}
\tag{5}\label{5}$$</div>
        </p>
<p>With this we compute a weight matrix consistent of normalized Gaussian distances:</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'w', 'portrait', 'def', false, '')", "id":"portrait-w_ij", "sym":"w", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {\mathit{w}} }_{\mathit{i},\mathit{j}} & = \frac{exp\left( -\frac{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'D', 'portrait', 'use', false, '')", "id":"portrait-D", "sym":"D", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{D}} }_{\mathit{i}, \mathit{j}}}{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'σ', 'portrait', 'use', false, '')", "id":"portrait-σ", "sym":"σ", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{σ}} }_{ \mathit{j} }} \right)}{\sum_{j^{\prime}} exp\left( -\frac{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'D', 'portrait', 'use', false, '')", "id":"portrait-D", "sym":"D", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{D}} }_{\mathit{i}, j^{\prime}}}{\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'σ', 'portrait', 'use', false, '')", "id":"portrait-σ", "sym":"σ", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{σ}} }_{ j^{\prime} }} \right)}\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['D_i,j', 'D_i,$j^{\\\\prime}$', 'σ', 'w'], false, []);"} }{}
\end{align*}
\tag{6}\label{6}$$</div>
        </p>
<p>Unlike a conventional normalized RBF kernel, $W_{i,j}$ using a different ${\proselabel{portrait}{{σ}}}$ for each of the j vertices. Each vertex’s ${\proselabel{portrait}{{σ}}}$ is selected such that each landmark’s influence in the kernel is inversely proportional to how many nearby neighbors it has for this particular image:</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'σ', 'portrait', 'def', false, '')", "id":"portrait-σ", "sym":"σ", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {\mathit{σ}} }_{ \mathit{j} } & = \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'select', 'portrait', 'use', false, '')", "id":"portrait-select", "sym":"select", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{select}} }\left( {\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'u', 'portrait', 'use', false, '')", "id":"portrait-u", "sym":"u", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{u}} }_{ \mathit{j} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'u', 'portrait', 'use', false, '')", "id":"portrait-u", "sym":"u", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{u}} }_{ j^{′} } \right)}^{2} + {\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'v', 'portrait', 'use', false, '')", "id":"portrait-v", "sym":"v", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{v}} }_{ \mathit{j} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'v', 'portrait', 'use', false, '')", "id":"portrait-v", "sym":"v", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {\mathit{v}} }_{ j^{′} } \right)}^{2},\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$K_σ$', 'portrait', 'use', false, '')", "id":"portrait-$K_σ$", "sym":"$K_σ$", "func":"portrait",  "localFunc":"", "type":"use", "case":"equation"} }{ {K_σ} } \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$K_σ$', 'u', 'select', 'v', 'σ'], false, []);"} }{}
\end{align*}
\tag{7}\label{7}$$</div>
        </p>
<p>Where <span sym='select' context='portrait'>  $ {\prosedeflabel{portrait}{{select}}} (·, K )$ returns the K’th smallest element of an input vector  </span>. This results in a warp where sparse keypoints have significant influence over their local neighborhood, while the influence of densely packed keypoints is diluted. This weight matrix is then used to compute the weighted average of mirrored vertex locations, and this 2D location is used to bilinearly interpolate into the input image to produce it’s mirrored equivalent:</p>
<p>$$\bar{I} = I\large( \sum_j W_{i,j} u_{\bar{j}}, \sum_j W_{i,j} v_{\bar{j}} \large) \tag{8}\label{8}$$</p>
<p>The only hyperparameter in this warping model is <span sym='K_σ' context='portrait'>  an integer value ${\prosedeflabel{portrait}{{K_σ}}}$, which we set to 4 in all experiments  </span>. This proposed warping model is robust to asymmetric expressions and poses assuming the landmarks are accurate, but is sensitive to asymmetric skin features, e.g., birthmarks.</p>
<p>The input to our facial shadow network is the concatenation of the input image ${\proselabel{portrait}{{I}}}$ with its mirrored version $\bar{I}$ along the channel dimension. This means that the receptive field of our CNN includes not just the local image neighborhood, but also its mirrored counterpart. Note that we do not include the mirrored image as input to our foreign shadow model, as we found it did not improve results. We suspect that this is due to the unconstrained nature of foreign shadow appearance, which weakens the assumption that corresponding face regions will have different lighting.</p>
<h1 id='neural-network-architecture-and-training'>4&nbsp;NEURAL NETWORK ARCHITECTURE AND TRAINING</h1><p>Here we describe the neural network architectures that we use for removing foreign shadows and for softening facial shadows. As the two tasks use different datasets and there is an additional conditional component in the facial shadow softening model, we train these two tasks separately.</p>
<p>For both models, we employ a GridNet [<a class="citation" href="#ref-fourure2017residual" id="cite-fourure2017residual">Fourure et al. 2017</a>] architecture with modifications proposed in [<a class="citation" href="#ref-niklaus2018context" id="cite-niklaus2018context">Niklaus and Liu 2018</a>]. GridNet is a grid-like architecture of rows and columns, where each row is a stream that processes features with resolution kept unchanged, and columns connect the streams by downsampling or upsampling the features. By allowing computation to happen at different layers and different spatial scales instead of conflating layers and spatial scales (as U-Nets do) GridNet produces more accurate predictions as has been successfully applied to a number of image synthesis tasks [<a class="citation" href="#ref-niklaus2018context" id="cite-niklaus2018context">Niklaus and Liu 2018</a>; <a class="citation" href="#ref-niklaus20193d" id="cite-niklaus20193d">Niklaus et al. 2019</a>]. We use a GridNet with eight columns wherein the first three columns perform downsampling and the remaining five columns perform upsampling, and use five rows for foreign model and six rows for facial model, as we found this to work best after an architecture search.</p>
<p>For all training samples, we run a face detector to obtain a face bounding box, then resize and crop the face into 256 × 256 resolution. For the foreign shadow removal model, the input to the network is a 3-channel RGB image and the output of the model is a 3-channel scaling A and a 3-channel offset B, which are then applied to the input to produce a 3-channel output image (Equation 1). For the facial shadow softening model, we additionally concatenate the input to the network with its mirrored counterpart (as per Section 3.3). As we would like our model to allow for a variable degree of shadow softening and of fill lighting intensity, we introduce two “knobs”—one for light size m and the other for fill light intensity $P_{fill}$, which are assumed to be provided as input. To inject this information into our network, a 2-channel image containing these two values at every pixel is concatenated into both the input and the last layers of the encoders of the network.</p>
<p>We supervise our two models using a <span sym='L_{pix}' context='portrait'>  weighted combination of pixel-space L1 loss (${\prosedeflabel{portrait}{{L_{pix}}}}$)   </span>and <span sym='L_{feat}' context='portrait'>  a perceptual feature space loss (${\prosedeflabel{portrait}{{L_{feat}}}}$) which has been used successfully to train models such as image synthesis and image decomposition   </span>[<a class="citation" href="#ref-chen2017photographic" id="cite-chen2017photographic">Chen and Koltun 2017</a>; <a class="citation" href="#ref-zhang2019synthetic" id="cite-zhang2019synthetic">Zhang et al. 2019</a>, <a class="citation" href="#ref-zhang2018single" id="cite-zhang2018single">Zhang et al. 2018</a>]. Intuitively, the perceptual loss accounts for high-level semantics in the reconstructed image but may be invariant to some non-semantic image content. By additionally minimizing a per-pixel L1 loss our model is better able to recover low-frequency image content. The perceptual loss is computed by processing the reconstructed and ground truth images through <span sym='Φ' context='portrait'>  a pre-trained VGG-19 network ${\prosedeflabel{portrait}{{Φ}}} (·)$   </span>and computing the L1 difference between extracted features in selected layers as specified in [<a class="citation" href="#ref-zhang2018single" id="cite-zhang2018single">Zhang et al. 2018</a>]. The final loss function is formulated as:</p>
<p>
        <div class='equation' code_block="portrait">
        $$\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{align*}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$L_{feat}$', 'portrait', 'def', false, '')", "id":"portrait-$L_{feat}$", "sym":"$L_{feat}$", "func":"portrait",  "localFunc":"", "type":"def", "case":"equation"} }{ {L_{feat}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, '$L_{feat}$')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right) & = \sum_{\mathit{d}} \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'λ', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-λ", "sym":"λ", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{λ}} }_{ \mathit{d} }\left\|\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'Φ', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-Φ", "sym":"Φ", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{Φ}} }_{ \mathit{d} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I^*$', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-$I^*$", "sym":"$I^*$", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {I^*} } \right) - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'Φ', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-Φ", "sym":"Φ", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{Φ}} }_{ \mathit{d} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'f', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-f", "sym":"f", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{f}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_{in}$', 'portrait', 'use', false, '$L_{feat}$')", "id":"portrait-$I_{in}$", "sym":"$I_{in}$", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {I_{in}} };\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, '$L_{feat}$')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right) \right)\right\|_F\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$I^*$', '$I_{in}$', 'θ', 'f', 'λ', 'Φ', '$L_{feat}$'], true, '$L_{feat}$', ['θ']);"} }{}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$L_{pix}$', 'portrait', 'def', false, '$L_{feat}$')", "id":"portrait-$L_{pix}$", "sym":"$L_{pix}$", "func":"portrait",  "localFunc":"$L_{feat}$", "type":"def", "case":"equation"} }{ {L_{pix}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, '$L_{pix}$')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right) & = \left\|\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I^*$', 'portrait', 'use', false, '$L_{pix}$')", "id":"portrait-$I^*$", "sym":"$I^*$", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"use", "case":"equation"} }{ {I^*} } - \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'f', 'portrait', 'use', false, '$L_{pix}$')", "id":"portrait-f", "sym":"f", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"use", "case":"equation"} }{ {\mathit{f}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$I_{in}$', 'portrait', 'use', false, '$L_{pix}$')", "id":"portrait-$I_{in}$", "sym":"$I_{in}$", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"use", "case":"equation"} }{ {I_{in}} };\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, '$L_{pix}$')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right)\right\|_F\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['$I_{in}$', '$I^*$', 'θ', 'f', '$L_{pix}$'], true, '$L_{pix}$', ['θ']);"} }{}
\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'L', 'portrait', 'def', false, '$L_{pix}$')", "id":"portrait-L", "sym":"L", "func":"portrait",  "localFunc":"$L_{pix}$", "type":"def", "case":"equation"} }{ {\mathit{L}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, 'L')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"L", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right) & = 0.01\idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$L_{feat}$', 'portrait', 'use', false, 'L')", "id":"portrait-$L_{feat}$", "sym":"$L_{feat}$", "func":"portrait",  "localFunc":"L", "type":"use", "case":"equation"} }{ {L_{feat}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, 'L')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"L", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right) + \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, '$L_{pix}$', 'portrait', 'use', false, 'L')", "id":"portrait-$L_{pix}$", "sym":"$L_{pix}$", "func":"portrait",  "localFunc":"L", "type":"use", "case":"equation"} }{ {L_{pix}} }\left( \idlabel{ {"onclick":"event.stopPropagation(); onClickSymbol(this, 'θ', 'portrait', 'use', true, 'L')", "id":"portrait-θ", "sym":"θ", "func":"portrait",  "localFunc":"L", "type":"use", "case":"equation"} }{ {\mathit{θ}} } \right)\\\eqlabel{ {"onclick":"event.stopPropagation(); onClickEq(this, 'portrait', ['θ', '$L_{pix}$', '$L_{feat}$', 'L'], true, 'L', ['θ']);"} }{}
\end{align*}
\tag{9}\label{9}$$</div>
        </p>
<p>where <span sym='I^*' context='portrait'>  ${\prosedeflabel{portrait}{{I^*}}}$ is the ground-truth shadow-removed or shadow-softened RGB image  </span>, <span sym='f' context='portrait'>  ${\prosedeflabel{portrait}{{f}}} (·; {\proselabel{portrait}{{θ}}} )$ denotes our neural network  </span>, and <span sym='λ' context='portrait'>  $ {\prosedeflabel{portrait}{{λ}}} _d$ denotes the selected weight for the d-th VGG layer  </span>. ${\proselabel{portrait}{{I_{in}}}} = {\proselabel{portrait}{{I}}}$ for foreign removal model and ${\proselabel{portrait}{{I_{in}}}} = concat( {\proselabel{portrait}{{I}}} , \bar{I} , P_{fill} , m )$ for facial shadow softening model. This same loss is used to train both models separately. We minimize ${\proselabel{portrait}{{L}}}$ with respect to both of our <span sym='θ' context='portrait'>  model weights ${\prosedeflabel{portrait}{{θ}}}$  </span> using Adam [<a class="citation" href="#ref-kingma2014adam" id="cite-kingma2014adam">Kingma and Ba 2014</a>]  ($β_1 = 0.9, β_2 = 0.999, ε = 10^{−8}$) for 500K iterations, with a learning rate of $10^{−4}$ that is decayed by a factor of 0.9 every 50K iterations.</p>
<h1 id='experiments'>5&nbsp;EXPERIMENTS</h1><p>We use synthetic and real in-the-wild test sets to evaluate our foreign shadow removal model (Section 5.3) and our facial shadow softening model (Section 5.4). We also present an ablation study of the components of our foreign shadow synthesis model (Section 5.2) as well as of our facial symmetry modeling. Extensive additional results can be found in the supplement.</p>
<h2 id='evaluation-data'>5.1&nbsp;Evaluation Data</h2><p>We evaluate our foreign shadow removal model with two datasets: (1) foreign-syn: We use a held-out set of the same synthetic data generation approach described in (Section 3.1), where the images (i.e., subjects) and shadow masks to generate test-set images are not present in the training set.</p>
<p>2) foreign-real: We collect an additional dataset of in-the-wild images for which we can obtain ground-truth images that do not contain foreign shadows. This dataset enables the quantitative and qualitative comparison of our proposed model against prior work. This is accomplished by capturing high-framerate (60 fps) videos of stationary human subjects while moving a shadow-casting object in front of the subject. We collect this evaluation dataset outdoors, and use the sun as the dominant light source. For each captured video, we manually identify a set of “lit” images and a set of “shadowed” images. For each “shadowed” image, we automatically use homography to align it to each “lit” and find the one that gives the minimum mean pixel error as its counterpart. Because the foreign object is moving during capture, this collection method provides a diversity in the shape and the position of foreign shadows (see examples in Figure 5 and the supplemental video). In total, we capture 20 videos of 8 subjects during different times of day, which gives us 100 image pairs of foreign shadow with ground truth.</p>
<figure>
<img src="./img/img5.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 5. An example of the shadow removal evaluation dataset we produce using video captured by a smartphone camera stabilized on a tripod. By filming a stationary subject under the shadow cast by a moving foreign occluder (a-c), we are able to obtain multiple input/ground-truth image pairs of the subject (a, c), (b, c). This provides us with an efficient way to collect a set of diverse foreign shadows for evaluation. Please see the supplemental video at 02:39 for more example video clips.
</figcaption> 
</figure>
<p>We evaluate our facial shadow model with another dataset: (3) facial-syn: We use the same OLAT Light Stage data that is used to generate our facial model training data to generate a test set, by using a held-out set of 5 subjects that are not used during training. We record each harsh input shadow image and the soft ground-truth output image along with their corresponding light size m and fill light intensity Pfill for use. Note that though this dataset is produced through algorithmic means, the ground-truth outputs are a weighted combination of real observed Light Stage images, and are therefore an accurate reflection of the true appearance of the subject up to the sampling limitations of the Light Stage hardware.</p>
<p>We qualitatively evaluate both our foreign shadow removal model and our facial shadow softening model using an additional dataset: (4) wild: We collect 100 “in the wild” portrait images of varied human subjects that contain a mix of different foreign and facial shadows. Images are taken from the Helen dataset [<a class="citation" href="#ref-le2012interactive" id="cite-le2012interactive">Le et al. 2012</a>], the HDRnet dataset [<a class="citation" href="#ref-gharbi2017deep" id="cite-gharbi2017deep">Gharbi et al. 2017</a>], and our own captures. These images are processed by our foreign shadow removal model, our facial shadow softening model, or both, to generate enhanced outputs that give a sense of the qualitative properties of both components of our model. See Figures 1, 8, 10, and the supplement for results.</p>
<h2 id='ablation-study-of-foreign-shadow-synthesis'>5.2&nbsp;Ablation Study of Foreign Shadow Synthesis</h2><p>Our foreign shadow synthesis technique (Section 3.1) simulates the complicated effect of foreign shadows on the appearance of human subjects. We evaluate this technique by removing each of the three components and measuring model performance. Our three ablations are: 1) “No-SV”: synthesis without spatially varying blur or the intensity variation of the shadow, 2) “No-SS”: synthesis where the approximated subsurface scattering of skin has been removed, and 3) “No-Color”: synthesis where the color perturbation to generate the shadow image is not randomly changed. Quantitative results for this ablation study on our foreign-syn and foreign-real datasets can be found in Table 1, and qualitative results for a test image from foreign-real are shown in Figure 6.</p>
<figure>
<img src="./img/img6.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 6. A visualization of an ablation study of our foreign shadow removal algorithm as different aspects of our foreign shadow synthesis model (Section 3.1) are removed. The “No-SV”, “No-SS”, and “No-Color” ablations show our model trained on synthesized data without modeling spatial variation, approximate subsurface scattering, or color perturbation, respectively. The top row shows the images generated by each model, and the bottom row shows the difference between each output and the ground truth image (f). Our complete model (e) clearly outperforms the others. Notice the red-colored residual along the shadow edge in the model trained without approximating subsurface scattering (c), and the color inconsistency in the removed region in the model trained without color perturbation (d). A quantitative evaluation on the entire set foreign-real is shown in Table 1.
</figcaption> 
</figure>
<figure>
<img src="./img/table1.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Table 1. A quantitative ablation study of our foreign shadow synthesis model in terms of PSNR, SSIM, and LPIPS. Ablating any component of our synthesis model hurts the performance of the resulting model.
</figcaption> 
</figure>
<figure>
<img src="./img/table2.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Table 2. A quantitative evaluation of our foreign shadow removal model. We compare against baseline methods of [<a class="citation" href="#ref-guo2012paired" id="cite-guo2012paired">Guo et al. 2012</a>], [<a class="citation" href="#ref-hu2019direction" id="cite-hu2019direction">Hu et al. 2019</a>] (SRD) and [<a class="citation" href="#ref-cun2020towards" id="cite-cun2020towards">Cun et al. 2020</a>] on both synthetic and real test sets. The input image itself is also included as point of reference. In terms of both image-quality (PSNR) and perceptual-quality (SSIM and LPIPS), our model produces better performance on all three metrics with a large margin. Visual comparisons can be seen in Figure 7.
</figcaption> 
</figure>
<figure>
<img src="./img/table3.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Table 3. A comparison of our facial shadow reduction model against the PR-net of [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>] and an ablation of our model with symmetry. in terms of PSNR, SSIM, and LPIPS on the “facial-syn” test dataset. We see that PR-net performs poorly on images that contain harsh facial shadows, and removing the concatenated “mirrored” input during training (i.e., setting Iin = I ) lowers accuracy by all three metrics.
</figcaption> 
</figure>
<h2 id='foreign-shadow-removal-quality'>5.3&nbsp;Foreign Shadow Removal Quality</h2><p>Because no prior work appears to address the task of foreign shadow removal for human faces, we compare our model against general-purpose shadow removal methods: the state-of-the-art learningbased method of [<a class="citation" href="#ref-cun2020towards" id="cite-cun2020towards">Cun et al. 2020</a>] 2 that uses a generative model to synthesize and then remove shadows, a customized network with attention mechanism designed by [<a class="citation" href="#ref-hu2019direction" id="cite-hu2019direction">Hu et al. 2019</a>] 3 for shadow detection and removal, and the non-learning-based method of [<a class="citation" href="#ref-guo2012paired" id="cite-guo2012paired">Guo et al. 2012</a>] that relies on image segmentation and graph cuts. The original implementation from [<a class="citation" href="#ref-guo2012paired" id="cite-guo2012paired">Guo et al. 2012</a>] is not available publicly, so we use a reimplementation4 that is able to reproduce the results of the original paper. We use the default parameters settings for this code, as we find that tuning its parameters did not improve performance for our task. [<a class="citation" href="#ref-hu2019direction" id="cite-hu2019direction">Hu et al. 2019</a>] provide two models trained on two general-purpose shadow removal benchmark datasets (SRD and ISTD), we use the SRD model as it performs better than the ISTD model on our evaluation dataset.</p>
<p>We evaluate these baseline methods on our foreign-syn and foreign-real datasets, as these both contain ground truth shadow-free images. We compute PSNR, SSIM [<a class="citation" href="#ref-wang2004image" id="cite-wang2004image">Wang et al. 2004</a>] and a learned perceptual metric LPIPS [<a class="citation" href="#ref-zhang2018unreasonable" id="cite-zhang2018unreasonable">Zhang et al. 2018</a>] between the ground truth and the output. Results are shown in Table 2 and Figure 7. Our model outperforms these baselines by a large margin. Please see video at 03:03 for more results.</p>
<figure>
<img src="./img/img7.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 7. Foreign shadow removal results on images from our foreign-real test dataset. The method of [<a class="citation" href="#ref-guo2012paired" id="cite-guo2012paired">Guo et al. 2012</a>] often incorrectly identifies dark image regions as shadows and removes them, while also failing to identify real shadows (b). The deep learning approaches of [<a class="citation" href="#ref-cun2020towards" id="cite-cun2020towards">Cun et al. 2020</a>] and [<a class="citation" href="#ref-hu2019direction" id="cite-hu2019direction">Hu et al. 2019</a>] (c, d) do a better job of correctly identifying shadow regions but often fail to remove shadows completely, and also change the overall brightness and tone of the image in a way that does not preserve the authenticity of the input image. Our method is able to entirely remove foreign shadows while still preserving the overall appearance of the subject (e), thereby producing output images that more closely resemble the ground truth (f).

</figcaption> 
</figure>
<figure>
<img src="./img/img8.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 8. Foreign shadow removal results of our model on our wild test dataset. Input images that contain unwanted foreign shadows (top) are processed by our foreign shadow removal model (bottom). Though real-world foreign shadows exhibit significant variety in terms of shape, softness, and color, our foreign shadow removal model is able to successfully generalize to these challenging real-world images despite having been trained entirely on our synthetic training data (Section 3.1).
</figcaption> 
</figure>
<figure>
<img src="./img/img9.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 9.  Facial shadow softening results on facial-syn. We compare against the portrait relighting model (PR-net) [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>] by applying a blur to its estimated environment light and relighting the input image with that blurred environment map. PR-net is able to successfully soften low frequency shadows but struggles with harsh facial shadows (b). The ablation of our model without our symmetry component (Section 3.3) also underperforms on these harsh facial shadows (c). Our complete model successfully softens these hard shadows (d), as it is able to reason about the bilateral symmetry of the subject and “borrow” pixels with similar reflectance and geometry from the opposite side of the face.
</figcaption> 
</figure>
<figure>
<img src="./img/img10.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 10. Facial shadow softening results on images from wild. Input images may contain harsh facial shadows, such as around the eyes (row 1) and by the subject’s cheek (row 3). Applying our facial shadow softening model with a variable “light size” m produces images with softer shadows (b, c). The specular reflection also gets suppressed, which is a desired photographic practice as specular highlights are often distracting and obscuring the surface of the subject. Additionally, the lighting ratio component of our model reduces the contrast induced by facial shadows (d) by adding a synthetic fill light with intensity Pfill, set here to the maximum value used in training (Section 3.2), in the direction opposite to the detected key light, as visualized in (e).
</figcaption> 
</figure>

<h2 id='facial-shadow-softening-quality'>5.4&nbsp;Facial Shadow Softening Quality</h2><p>Transforming harsh facial shadows to soft in image space is roughly equivalent to relighting a face with a blurred version of the dominant light source in the original lighting environment. We compare our facial softening model against the portrait relighting method from [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>], by applying a Gaussian blur to the estimated environment map from the model and then pass to the decoder for relighting. The amount of blur to apply, however, cannot map exactly to our light size parameter. We experiment with a few blur kernel values and choose the one that produces the minimum mean pixel error with the ground truth. We do this for each image, and show qualitative comparisons in Figure 10. In Table 3, we compare our model against the [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>] baseline and against an ablation of our model without symmetry, and demonstrate an improvement with respect to both. For all comparisons, we use facial-syn, which has ground truth soft facial shadows. Please see video at 03:35 for more results.</p>
<h2 id='preprocessing-for-portrait-relighting'>5.5&nbsp;Preprocessing for Portrait Relighting</h2><p>Our method can also be used as a “preprocessing” step for image modification algorithms such as portrait relighting [<a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a>; <a class="citation" href="#ref-zhou2019deep" id="cite-zhou2019deep">Zhou et al. 2019</a>], which modify or replace the illumination of the input image. Though often effective, these portrait relighting techniques sometimes produce suboptimal renderings when presented with input images that contain foreign shadows or harsh facial shadows. Our technique can improve a portrait relighting solution: our model can be used to remove these unwanted shadowing effects, producing a rendering that can then be used as input to a portrait relighting solution, resulting in an improved final rendering. See Figure 11 for an example.</p>
<figure>
<img src="./img/img11.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 11. The portrait relighting technique of <a class="citation" href="#ref-sun2019single" id="cite-sun2019single">Sun et al. 2019</a> provides an alternative approach for shadow manipulation. However, applying this technique to input images that contain foreign shadows and harsh facial shadows (a) often results in relit images in which these foreign and facial shadows persist as artifacts (b). If this same portrait relighting technique is instead applied to the output images of our model (c), it produces a less jarring (though still somewhat suboptimal) rendering of the subject (d).
</figcaption> 
</figure>
<h1 id='limitations'>6&nbsp;LIMITATIONS</h1><p>Our proposed model is not without its limitations, some of which we can identify in our wild dataset. When foreign shadows contain many finely-detailed structures (which are underrepresented in training), our output may retain visible residuals of those (Figure 12(a)). While exploiting the bilateral symmetry of the subject significantly improves our facial softening model’s performance, this causes our model to sometimes fail to remove shadows that also happen to be bilaterally symmetric (Figure 12(b)). Because the training data of our shadow softening model is rendered by increasing the light size—a simple lighting setup that introduces bias towards generating diffused-looking images. For example, when the “light size” is set high in Figure 10 (c), our shadow softening model generates images with a “flat” appearance and smooths out high frequency details in the hair regions that could have been preserved if different lighting setups are used for face and hair during training data generation.</p>
<figure>
<img src="./img/img12.png" alt="Trulli" style="width:100%" class = "center">
<figcaption align = "center">Fig. 12. Example failure cases from our wild dataset. We notice limitations of our foreign shadow removal model in handling fine-detailed structures (a), of our facial shadow softening model reducing highly facial shadows (b), and of the models not correctly separating the two types of shadows (c).
</figcaption> 
</figure>
<p>Our model assumes that shadows belong to one of two categories (“foreign” and “facial”) but these two categories are not always entirely distinct and easily-separated. Because of this, sufficiently harsh facial shadows may be erroneously detected and removed by our foreign shadow removal model (Figure 12(c)). This suggests that our model may benefit from a unified approach for both kinds of shadows, though this approach is somewhat at odds with the constraints provided by image formation and our datasets: a unified learning approach would require a unified source of training data, and it is not clear how existing light stage scans or in-the-wild photographs could be used to construct a large, diverse, and photorealistic dataset in which both foreign and facial shadows are present and available as ground-truth.</p>
<p>Constructing a real-world dataset for our task that contains ground-truth is challenging. Though the foreign-real dataset used for qualitatively evaluating our foreign shadow removal algorithm is sufficiently diverse and accurate to evaluate different algorithms, it has some shortcomings. This dataset is not large enough to be used for training, and does not provide a means for evaluating facial shadow softening. This dataset also assumes that all foreign shadows are cast by a single occluder blocking the light of a single dominant illuminant, while real-world instances of foreign shadows often involve multiple illuminants and occluders. Additionally, to satisfy our single-illuminant assumption, this dataset had to be captured in real-world environments that have one dominant light source (e.g. , outdoors in the midday sun). This gave us little control over the lighting environment, and resulted in images with high dynamic ranges and therefore “deep” dark shadows, which may degrade (via noise and quantization) image content in shadowed regions. A real-world dataset that addresses these issues be invaluable for evaluating and improving portrait shadow manipulation algorithms.</p>
<h1 id='conclusion'>7&nbsp;CONCLUSION</h1><p>We have presented a new approach for enhancing casual portrait photographs with respect to lighting and shadows, particularly in terms of foreign shadow removal, facial shadow softening, and lighting ratio balancing. When synthesizing training data the foreign shadow removal task, we observe the value of in-the-wild images with a shadow synthesis model that accounts for the irregularity of foreign shadows in the real world. Motivated by the physical tools used by photographers in studio environments, we have demonstrated how Light Stage scans can be used to produce training data for facial shadow softening. We have presented a mechanism for allowing convolutional neural networks to exploit the inherent bilateral symmetry of human subjects, and we have demonstrated that this improves the performance of facial shadow softening. Given just a single image of a human subject taken in an unknown and unconstrained environment, our complete system is able to remove unwanted foreign shadows, soften harsh facial shadows, and balance the image’s lighting ratio to produce a flattering and realistic portrait image.</p>
<h1 id='acknowledgements'>8&nbsp;ACKNOWLEDGEMENTS</h1><p>We thank Augmented Perception from Google for collecting the light stage data. Ren Ng is supported by a Google Research Grant. This work is also supported by a number of individuals from various aspects. We thank Marc Levoy, Kevin (Jiawen) Chen and anonymous reviewers for helpful discussions on the paper. We thank Michael Milne and Andrew Radin for insightful discussions on professional lighting principles and practice. We thank Timothy Brooks for demonstrating Photoshop portrait shadow editing, and to Xiaowei Hu for running the baseline algorithm. We are also grateful to people who kindly consent to be in the video and result images.</p>
<h1 id='reference'>9&nbsp;REFERENCE</h1><div class="references">
<table>
<tbody>
<tr id="ref-grey2014master">
<td><p>Christopher Grey. 2014. Master lighting guide for portrait photographers. </p></td>
</tr>
<tr id="ref-rensink2004influence">
<td><p>Ronald A. Rensink and Patrick Cavanagh. 2004. The influence of cast shadows on visual search. <i>Perception</i> 33, (2004) </p></td>
</tr>
<tr id="ref-debevec2000acquiring">
<td><p>Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin and Mark Sagar. 2000. Acquiring the reflectance field of a human face. </p></td>
</tr>
<tr id="ref-shashua2001quotient">
<td><p>Amnon Shashua and Tammy Riklin-Raviv. 2001. The quotient image: Class-based re-rendering and recognition with varying illuminations. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> 23, (2001) </p></td>
</tr>
<tr id="ref-he2016deep">
<td><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. 2016. Deep residual learning for image recognition. </p></td>
</tr>
<tr id="ref-horn1974determining">
<td><p>Berthold KP. Horn. 1974. Determining lightness from an image. <i>Computer graphics and image processing</i> 3, (1974) </p></td>
</tr>
<tr id="ref-arbel2010shadow">
<td><p>Eli Arbel and Hagit Hel-Or. 2010. Shadow removal using intensity surfaces and texture anchor points. <i>IEEE transactions on pattern analysis and machine intelligence</i> 33, (2010) </p></td>
</tr>
<tr id="ref-gryka2015learning">
<td><p>Maciej Gryka, Michael Terry and Gabriel J. Brostow. 2015. Learning to remove soft shadows. <i>ACM Transactions on Graphics (TOG)</i> 34, (2015) </p></td>
</tr>
<tr id="ref-shor2008shadow">
<td><p>Yael Shor and Dani Lischinski. 2008. The shadow meets the mask: Pyramid-based shadow removal. </p></td>
</tr>
<tr id="ref-wu2007natural">
<td><p>Tai-Pang Wu, Chi-Keung Tang, Michael S. Brown and Heung-Yeung Shum. 2007. Natural shadow matting. <i>ACM Transactions on Graphics (TOG)</i> 26, (2007) </p></td>
</tr>
<tr id="ref-sato2003illumination">
<td><p>Imari Sato, Yoichi Sato and Katsushi Ikeuchi. 2003. Illumination from shadows. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> 25, (2003) </p></td>
</tr>
<tr id="ref-baba2003shadow">
<td><p>Masashi Baba and Naoki Asada. 2003. Shadow removal from a real picture. </p></td>
</tr>
<tr id="ref-kaufman2012content">
<td><p>Liad Kaufman, Dani Lischinski and Michael Werman. 2012. Content-Aware Automatic Photo Enhancement. </p></td>
</tr>
<tr id="ref-guo2012paired">
<td><p>Ruiqi Guo, Qieyun Dai and Derek Hoiem. 2012. Paired regions for shadow detection and removal. <i>IEEE transactions on pattern analysis and machine intelligence</i> 35, (2012) </p></td>
</tr>
<tr id="ref-ma2016appearance">
<td><p>Li-Qian Ma, Jue Wang, Eli Shechtman, Kalyan Sunkavalli and Shi-Min Hu. 2016. Appearance harmonization for single image shadow removal. </p></td>
</tr>
<tr id="ref-finlayson2009entropy">
<td><p>Graham D. Finlayson, Mark S. Drew and Cheng Lu. 2009. Entropy minimization for shadow removal. <i>International Journal of Computer Vision</i> 85, (2009) </p></td>
</tr>
<tr id="ref-finlayson2002removing">
<td><p>Graham D. Finlayson, Steven D. Hordley and Mark S. Drew. 2002. Removing shadows from images. </p></td>
</tr>
<tr id="ref-ramamoorthi2001signal">
<td><p>Ravi Ramamoorthi and Pat Hanrahan. 2001. A signal-processing framework for inverse rendering. </p></td>
</tr>
<tr id="ref-sengupta2018sfsnet">
<td><p>Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo and David W. Jacobs. 2018. Sfsnet: Learning shape, reflectance and illuminance of facesin the wild&rsquo;. </p></td>
</tr>
<tr id="ref-barron2014shape">
<td><p>Jonathan T. Barron and Jitendra Malik. 2014. Shape, illumination, and reflectance from shading. <i>IEEE transactions on pattern analysis and machine intelligence</i> 37, (2014) </p></td>
</tr>
<tr id="ref-grosse2009ground">
<td><p>Roger Grosse, Micah K. Johnson, Edward H. Adelson and William T. Freeman. 2009. Ground truth dataset and baseline evaluations for intrinsic image algorithms. </p></td>
</tr>
<tr id="ref-cun2020towards">
<td><p>Xiaodong Cun, Chi-Man Pun and Cheng Shi. 2020. Towards ghost-free shadow removal via dual hierarchical aggregation network and shadow matting GAN. </p></td>
</tr>
<tr id="ref-ding2019argan">
<td><p>Bin Ding, Chengjiang Long, Ling Zhang and Chunxia Xiao. 2019. ARGAN: Attentive recurrent generative adversarial network for shadow detection and removal. </p></td>
</tr>
<tr id="ref-hu2019direction">
<td><p>Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin and Pheng-Ann Heng. 2019. Direction-aware spatial context features for shadow detection and removal. <i>IEEE transactions on pattern analysis and machine intelligence</i> 42, (2019) </p></td>
</tr>
<tr id="ref-hu2018direction">
<td><p>Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin and Pheng-Ann Heng. 2018. Direction-aware spatial context features for shadow detection. </p></td>
</tr>
<tr id="ref-khan2015automatic">
<td><p>Salman H. Khan, Mohammed Bennamoun, Ferdous Sohel and Roberto Togneri. 2015. Automatic shadow detection and removal from a single image. <i>IEEE transactions on pattern analysis and machine intelligence</i> 38, (2015) </p></td>
</tr>
<tr id="ref-zheng2019distraction">
<td><p>Quanlong Zheng, Xiaotian Qiao, Ying Cao and Rynson WH. Lau. 2019. Distraction-aware shadow detection. </p></td>
</tr>
<tr id="ref-zhu2018bidirectional">
<td><p>Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin and Pheng-Ann Heng. 2018. Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. </p></td>
</tr>
<tr id="ref-qu2017deshadownet">
<td><p>Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang and Rynson WH. Lau. 2017. Deshadownet: A multi-context embedding deep network for shadow removal. </p></td>
</tr>
<tr id="ref-wang2018stacked">
<td><p>Jifeng Wang, Xiang Li and Jian Yang. 2018. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. </p></td>
</tr>
<tr id="ref-donner2006spectral">
<td><p>Craig Donner and Henrik Wann. Jensen. 2006. A Spectral BSSRDF for Shading Human Skin.. <i>Rendering techniques</i> 2006, (2006) </p></td>
</tr>
<tr id="ref-ulyanov2018deep">
<td><p>Dmitry Ulyanov, Andrea Vedaldi and Victor Lempitsky. 2018. Deep image prior. </p></td>
</tr>
<tr id="ref-yeh2017semantic">
<td><p>Raymond A. Yeh, Chen Chen, Teck Yian, Alexander G. Schwing, Mark Hasegawa-Johnson and Minh N. Do. 2017. Semantic image inpainting with deep generative models. </p></td>
</tr>
<tr id="ref-chen2018fsrnet">
<td><p>Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen and Jian Yang. 2018. Fsrnet: End-to-end learning face super-resolution with facial priors. </p></td>
</tr>
<tr id="ref-denton2015deep">
<td><p>Emily Denton, Soumith Chintala, Arthur Szlam and Rob Fergus. 2015. Deep generative image models using a laplacian pyramid of adversarial networks. <i>arXiv preprint arXiv:1506.05751</i>, (2015) </p></td>
</tr>
<tr id="ref-sun2019single">
<td><p>Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul E. Debevec and Ravi Ramamoorthi. 2019. Single image portrait relighting.. <i>ACM Trans. Graph.</i> 38, (2019) </p></td>
</tr>
<tr id="ref-zhou2019deep">
<td><p>Hao Zhou, Sunil Hadap, Kalyan Sunkavalli and David W. Jacobs. 2019. Deep single-image portrait relighting. </p></td>
</tr>
<tr id="ref-shih2014style">
<td><p>YiChang Shih, Sylvain Paris, Connelly Barnes, William T. Freeman and Fr{'e}do Durand. 2014. Style transfer for headshot portraits. </p></td>
</tr>
<tr id="ref-shu2017portrait">
<td><p>Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris and Dimitris Samaras. 2017. Portrait lighting transfer using a mass transport approach. <i>ACM Transactions on Graphics (TOG)</i> 36, (2017) </p></td>
</tr>
<tr id="ref-huang2000statistics">
<td><p>Jinggang Huang, Ann B. Lee and David Mumford. 2000. Statistics of range images. </p></td>
</tr>
<tr id="ref-hanrahan1993reflection">
<td><p>Pat Hanrahan and Wolfgang Krueger. 1993. Reflection from layered surfaces due to subsurface scattering. </p></td>
</tr>
<tr id="ref-jensen2001practical">
<td><p>Henrik Wann. Jensen, Stephen R. Marschner, Marc Levoy and Pat Hanrahan. 2001. A practical model for subsurface light transport. </p></td>
</tr>
<tr id="ref-krishnaswamy2004biophysically">
<td><p>Aravind Krishnaswamy and Gladimir VG. Baranoski. 2004. A biophysically-based spectral model of light interaction with human skin. </p></td>
</tr>
<tr id="ref-fernando2004gpu">
<td><p>Randima Fernando and  others. 2004. GPU gems: programming techniques, tips, and tricks for real-time graphics. </p></td>
</tr>
<tr id="ref-zhang2019effective">
<td><p>Ling Zhang, Qingan Yan, Yao Zhu, Xiaolong Zhang and Chunxia Xiao. 2019. Effective shadow removal via multi-scale image decomposition. <i>The Visual Computer</i> 35, (2019) </p></td>
</tr>
<tr id="ref-kartynnik2019real">
<td><p>Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko and Matthias Grundmann. 2019. Real-time facial surface geometry from monocular video on mobile GPUs. <i>arXiv preprint arXiv:1907.06724</i>, (2019) </p></td>
</tr>
<tr id="ref-fourure2017residual">
<td><p>Damien Fourure, R{'e}mi Emonet, Elisa Fromont, Damien Muselet, Alain Tremeau and Christian Wolf. 2017. Residual conv-deconv grid network for semantic segmentation. <i>arXiv preprint arXiv:1707.07958</i>, (2017) </p></td>
</tr>
<tr id="ref-niklaus2018context">
<td><p>Simon Niklaus and Feng Liu. 2018. Context-aware synthesis for video frame interpolation. </p></td>
</tr>
<tr id="ref-niklaus20193d">
<td><p>Simon Niklaus, Long Mai, Jimei Yang and Feng Liu. 2019. 3d ken burns effect from a single image. <i>ACM Transactions on Graphics (TOG)</i> 38, (2019) </p></td>
</tr>
<tr id="ref-chen2017photographic">
<td><p>Qifeng Chen and Vladlen Koltun. 2017. Photographic image synthesis with cascaded refinement networks. </p></td>
</tr>
<tr id="ref-zhang2019synthetic">
<td><p>Xuaner Zhang, Kevin Matzen, Vivien Nguyen, Dillon Yao, You Zhang and Ren Ng. 2019. Synthetic defocus and look-ahead autofocus for casual videography. <i>arXiv preprint arXiv:1905.06326</i>, (2019) </p></td>
</tr>
<tr id="ref-zhang2018single">
<td><p>Xuaner Zhang, Ren Ng and Qifeng Chen. 2018. Single image reflection removal with perceptual losses. </p></td>
</tr>
<tr id="ref-kingma2014adam">
<td><p>Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <i>arXiv preprint arXiv:1412.6980</i>, (2014) </p></td>
</tr>
<tr id="ref-le2012interactive">
<td><p>Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Bourdev and Thomas S. Huang. 2012. Interactive facial feature localization. </p></td>
</tr>
<tr id="ref-gharbi2017deep">
<td><p>Micha{"e}l Gharbi, Jiawen Chen, Jonathan T. Barron, Samuel W. Hasinoff and Fr{'e}do Durand. 2017. Deep bilateral learning for real-time image enhancement. <i>ACM Transactions on Graphics (TOG)</i> 36, (2017) </p></td>
</tr>
<tr id="ref-wang2004image">
<td><p>Zhou Wang, Alan C. Bovik, Hamid R. Sheikh and Eero P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. <i>IEEE transactions on image processing</i> 13, (2004) </p></td>
</tr>
<tr id="ref-zhang2018unreasonable">
<td><p>Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. </p></td>
</tr>
</tbody>
</table>
</div>
</body>
</html>